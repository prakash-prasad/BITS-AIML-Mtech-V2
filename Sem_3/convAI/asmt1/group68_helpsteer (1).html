<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>381375034a424d378c18cbb5cfc71a64</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div id="uQCecBMPrrxx" class="cell markdown" id="uQCecBMPrrxx">
<p>Members :</p>
<ol>
<li>PRAKASH PRASAD - <a href="/cdn-cgi/l/email-protection#81b3b1b3b3e0e2b1b4b3b4b7c1f6e8edf1afe3e8f5f2acf1e8ede0efe8afe0e2afe8ef" class="email"><span class="__cf_email__" data-cfemail="7d4f4d4f4f1c1e4d484f484b3d0a14110d531f14090e500d14111c1314531c1e531413">[email&#160;protected]</span></a></li>
<li>Mulla Akib Javed - <a href="/cdn-cgi/l/email-protection#88bab8babae9ebb8bdbebbb9c8ffe1e4f8a6eae1fcfba5f8e1e4e9e6e1a6e9eba6e1e6" class="email"><span class="__cf_email__" data-cfemail="5c6e6c6e6e3d3f6c696a6f6d1c2b35302c723e35282f712c35303d3235723d3f723532">[email&#160;protected]</span></a></li>
<li>AMIT KUMAR SRIVASTAVA - <a href="/cdn-cgi/l/email-protection#93a1a3a1a1f2f0a3a6a1aaa1d3e4faffe3bdf1fae7e0bee3fafff2fdfabdf2f0bdfafd" class="email"><span class="__cf_email__" data-cfemail="576567656536346762656e6517203e3b2779353e23247a273e3b36393e793634793e39">[email&#160;protected]</span></a></li>
<li>MRUNAL MADHUKAR AHIRE - <a href="/cdn-cgi/l/email-protection#3a080a08085b590a0f0808027a4d53564a1458534e49174a53565b5453145b59145354" class="email"><span class="__cf_email__" data-cfemail="4c7e7c7e7e2d2f7c797e7e740c3b25203c622e25383f613c25202d2225622d2f622522">[email&#160;protected]</span></a></li>
</ol>
<p>Dataset link : <a
href="https://huggingface.co/datasets/nvidia/HelpSteer"
class="uri">https://huggingface.co/datasets/nvidia/HelpSteer</a></p>
</div>
<div id="3d3d14dc" class="cell code" data-execution_count="2"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="3d3d14dc" data-outputId="a01a79ff-7567-49f1-c671-8d9ec4e2311b">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>huggingface<span class="op">/</span>transformers.git<span class="op">@</span>main git<span class="op">+</span>https:<span class="op">//</span>github.com<span class="op">/</span>huggingface<span class="op">/</span>peft.git <span class="op">-</span>q</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install bitsandbytes datasets accelerate loralib <span class="op">-</span>q</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu11.8/torch_stable.html</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>  Installing build dependencies ... ents to build wheel ... etadata (pyproject.toml) ... ents to build wheel ... etadata (pyproject.toml) ... </code></pre>
</div>
</div>
<div id="fac15894" class="cell code" data-execution_count="3"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:145,&quot;referenced_widgets&quot;:[&quot;41ead772886042e6a4618c8e8470cbd0&quot;,&quot;7222b9aefe0742efa780073b2638a500&quot;,&quot;f6c23c1c90cb492c8bd4827b46bd6eb5&quot;,&quot;ed3cc81d4e3a46ea933ee62b8d67a36b&quot;,&quot;3b61c22c2f214b0cbfa1037716c2c481&quot;,&quot;8fd9e25495fc49e689676c0fe44974f6&quot;,&quot;2dd27de4511d4ae498bc9b83b4144d79&quot;,&quot;b9af77b612ac40b484d3e4841f0237a1&quot;,&quot;2e77bff7b2604ca18dd68fd9bea638b4&quot;,&quot;c637ed866abf4680834c2481a7688f02&quot;,&quot;fe41e7f378864be9a10aad899d302913&quot;,&quot;955eb5fbaf30487cb7420a9b449c748b&quot;,&quot;610e3e347b64480badfe35dc07ad17bd&quot;,&quot;b1ecd3158ee946fbadb69ecd24d24b54&quot;,&quot;8ed3f4a0d90444d39af0a583d51e37a3&quot;,&quot;07cd51ff6aba4780b8ed39b335be0bd6&quot;,&quot;6f26853a5db2400699a2ad012d014b0f&quot;,&quot;4750a75ad7c246318d76daef8a436c36&quot;,&quot;34b2d7bfc3bd4cfd9bc2284690e4ee09&quot;,&quot;592777e5f2a74011841e58f1a12379fa&quot;,&quot;9f3b70d75614442c9a954d0a35d6835b&quot;,&quot;3d57aa43626349c0b1098fa3184494da&quot;,&quot;f883f951a1644bccbce0d7b95e952f9e&quot;,&quot;d0808c9d9eb8449a83fa9b614e39e251&quot;,&quot;166a16d1be7442bf9710c703e0e8d346&quot;,&quot;9d467d8156d6496ab514509e4f343999&quot;,&quot;0e47844c23a84f3299a9f5d01a44c767&quot;,&quot;ec09dc653b484aa3aafbfd3824a61504&quot;,&quot;ed0a979d5bdd4d96b342eb18af419a16&quot;,&quot;3dbf5e1ccb5d48c3b9df5a8a946c8442&quot;,&quot;135e658a56554244bb39c2693e1e43cb&quot;,&quot;32ebcf4ed049435086354414a8b953a8&quot;]}"
id="fac15894" data-outputId="0ba6b947-92a4-4db9-a38d-2c73c82c5926">
<div class="output display_data">
<div class="sourceCode" id="cb4"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;41ead772886042e6a4618c8e8470cbd0&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div id="739ea420" class="cell code" data-execution_count="4"
id="739ea420">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">&quot;ignore&quot;</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data.utils <span class="im">import</span> get_tokenizer</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> build_vocab_from_iterator</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.rnn <span class="im">import</span> pad_sequence</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># from torch.nn.parallel import DistributedDataParallel as DDP</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># from accelerate import Accelerator</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># from accelerate.utils import write_basic_config</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoConfig, AutoModelForCausalLM, RobertaTokenizer, GPT2Tokenizer, <span class="op">\</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM, AdamW</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer, models, trainers, pre_tokenizers, decoders, processors</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.random.manual_seed(0)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.cuda.set_device(0)</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co"># # torch.cuda.set_device(1)</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># # torch.cuda.set_device(2)</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co"># # torch.cuda.set_device(3)</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.cuda.is_available()</span></span></code></pre></div>
</div>
<div id="1a6255b2" class="cell code" data-execution_count="5"
id="1a6255b2">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code></pre></div>
</div>
<div id="73e0129f" class="cell code" id="73e0129f">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">&quot;PYTORCH_CUDA_ALLOC_CONF&quot;</span>] <span class="op">=</span> <span class="st">&quot;expandable_segments:True&quot;</span></span></code></pre></div>
</div>
<div id="69089fb6" class="cell code" id="69089fb6">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load dataset</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> load_dataset(<span class="st">&quot;nvidia/HelpSteer2&quot;</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> ds[<span class="st">&#39;train&#39;</span>] <span class="co"># len(train) = 20324 (95%)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>val <span class="op">=</span> ds[<span class="st">&#39;validation&#39;</span>]     <span class="co"># len(val) = 1038 (5%)</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>train_pd <span class="op">=</span> pd.DataFrame(train)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>val_pd <span class="op">=</span> pd.DataFrame(val)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>train_pd.head()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>val_pd.head()</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> train_pd.copy()</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> val_pd.copy()</span></code></pre></div>
</div>
<div id="fa142bb8" class="cell markdown" id="fa142bb8">
<h2
id="1-no-preprocessing-needed-for-llm--lm-preprocessing-for-lstm-1-mark">1.
no preprocessing needed for llm , lm preprocessing for lstm: [1
Mark]</h2>
</div>
<div id="5c678645" class="cell code" id="5c678645">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train_pd <span class="op">=</span> train_pd[:<span class="dv">500</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>val_pd <span class="op">=</span> val_pd[:<span class="dv">50</span>]</span></code></pre></div>
</div>
<div id="5171e605" class="cell code" id="5171e605"
data-outputId="0f3163eb-68be-4879-ad3b-c6c089d99398">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the prompts and answers</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> get_tokenizer(<span class="st">&quot;basic_english&quot;</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> yield_tokens(data_iter):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text <span class="kw">in</span> data_iter:</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> tokenizer(text)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Build vocabulary</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> build_vocab_from_iterator(yield_tokens(train_pd[<span class="st">&#39;prompt&#39;</span>].tolist() <span class="op">+</span> train_pd[<span class="st">&#39;response&#39;</span>].tolist()),</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                                  specials<span class="op">=</span>[<span class="st">&quot;&lt;unk&gt;&quot;</span>])</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>vocab.set_default_index(vocab[<span class="st">&quot;&lt;unk&gt;&quot;</span>])</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert texts to sequences</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> text_pipeline(text):</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vocab(tokenizer(text))</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>train_prompt_sequences <span class="op">=</span> [text_pipeline(text) <span class="cf">for</span> text <span class="kw">in</span> train_pd[<span class="st">&#39;prompt&#39;</span>]]</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>train_answer_sequences <span class="op">=</span> [text_pipeline(text) <span class="cf">for</span> text <span class="kw">in</span> train_pd[<span class="st">&#39;response&#39;</span>]]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>val_prompt_sequences <span class="op">=</span> [text_pipeline(text) <span class="cf">for</span> text <span class="kw">in</span> val_pd[<span class="st">&#39;prompt&#39;</span>]]</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>val_answer_sequences <span class="op">=</span> [text_pipeline(text) <span class="cf">for</span> text <span class="kw">in</span> val_pd[<span class="st">&#39;response&#39;</span>]]</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>max_len_prompt <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(seq) <span class="cf">for</span> seq <span class="kw">in</span> train_prompt_sequences)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>max_len_answer <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(seq) <span class="cf">for</span> seq <span class="kw">in</span> train_answer_sequences)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;max token length of prompts and response: &#39;</span>, max_len_prompt, max_len_answer)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Vocabulary size</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Vocabulary size from train prompt + response &#39;</span>, vocab_size)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad the sequences</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pad_sequences(sequences, max_len):</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pad_sequence([torch.tensor(seq[:max_len]) <span class="cf">for</span> seq <span class="kw">in</span> sequences], batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>                        padding_value<span class="op">=</span>vocab[<span class="st">&quot;&lt;unk&gt;&quot;</span>])</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>train_prompt_sequences <span class="op">=</span> pad_sequences(train_prompt_sequences, max_len_prompt)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>train_answer_sequences <span class="op">=</span> pad_sequences(train_answer_sequences, max_len_answer)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>val_prompt_sequences <span class="op">=</span> pad_sequences(val_prompt_sequences, max_len_prompt)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>val_answer_sequences <span class="op">=</span> pad_sequences(val_answer_sequences, max_len_answer)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>max token length of prompts and response:  548 1045
Vocabulary size from train prompt + response  7095
</code></pre>
</div>
</div>
<div id="777b6a12" class="cell code" id="777b6a12"
data-outputId="ac4d7267-497d-4754-847c-0af4fab7bfc5">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>train_prompt_sequences.shape, train_answer_sequences.shape, val_prompt_sequences.shape, val_answer_sequences.shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="14">
<pre><code>(torch.Size([300, 548]),
 torch.Size([300, 1045]),
 torch.Size([50, 387]),
 torch.Size([50, 808]))</code></pre>
</div>
</div>
<div id="52e8b088" class="cell raw" id="52e8b088">

</div>
<div id="241f5d9c" class="cell markdown" id="241f5d9c">
<h2
id="2explain-and-implement-how-llm-2b-to-8b-parameters-over-simple-lm-can-help-improve-the-system-accuracy-1-mark">2.Explain
and implement how LLM (2B to 8B parameters) over simple LM can help
improve the system accuracy. [1 Mark]</h2>
</div>
<div id="12573dc5" class="cell code" id="12573dc5">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seq 2 Seq LM using lstm on Pytorch</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2Seq(nn.Module):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, hidden_units):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Seq2Seq, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_lstm <span class="op">=</span> nn.LSTM(embedding_dim, hidden_units, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_lstm <span class="op">=</span> nn.LSTM(embedding_dim, hidden_units, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(hidden_units, vocab_size)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, encoder_input, decoder_input, hidden):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        encoder_embedded <span class="op">=</span> <span class="va">self</span>.embedding(encoder_input)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        encoder_output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.encoder_lstm(encoder_embedded, hidden)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        decoder_embedded <span class="op">=</span> <span class="va">self</span>.embedding(decoder_input)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        decoder_output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.decoder_lstm(decoder_embedded, (hidden, cell))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.fc(decoder_output)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, (hidden, cell)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>hidden_units <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2Seq(vocab_size, embedding_dim, hidden_units)</span></code></pre></div>
</div>
<div id="6a9b8b5a" class="cell code" id="6a9b8b5a">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the loss and optimizer</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss(ignore_index<span class="op">=</span>vocab[<span class="st">&quot;&lt;unk&gt;&quot;</span>])</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>device<span class="op">=</span><span class="st">&#39;cpu&#39;</span> <span class="co"># using cpu since gpu gives memory error</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Move model to GPU</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># device_ids = [0, 1, 2, 3]  # Replace with desired GPU IDs</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># model = nn.DataParallel(model, device_ids=device_ids)</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># model.to(f&#39;cuda:{device_ids[0]}&#39;)  # Move the model to the first GPU</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># # input_tensor = input_tensor.to(device)</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># model = model.to(device)</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># scaler = torch.cuda.amp.GradScaler()</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the target data for the decoder (shifted one time step) !!!</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_target_sequences(sequences):</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> sequences[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> sequences[:, <span class="dv">1</span>:]</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> inputs, targets</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>train_answer_sequences_input, train_answer_sequences_output <span class="op">=</span> prepare_target_sequences(train_answer_sequences)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>val_answer_sequences_input, val_answer_sequences_output <span class="op">=</span> prepare_target_sequences(val_answer_sequences)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to PyTorch tensors and move to device</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>train_prompt_sequences <span class="op">=</span> train_prompt_sequences.to(device)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>train_answer_sequences_input <span class="op">=</span> train_answer_sequences_input.to(device)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>train_answer_sequences_output <span class="op">=</span> train_answer_sequences_output.to(device)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>val_prompt_sequences <span class="op">=</span> val_prompt_sequences.to(device)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>val_answer_sequences_input <span class="op">=</span> val_answer_sequences_input.to(device)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>val_answer_sequences_output <span class="op">=</span> val_answer_sequences_output.to(device)</span></code></pre></div>
</div>
<div id="6819aee1" class="cell code" id="6819aee1"
data-outputId="dde9b6c8-4251-473b-e967-ede81dbb370b">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> time.time()</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    output, _ <span class="op">=</span> model(train_prompt_sequences, train_answer_sequences_input, <span class="va">None</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     loss = criterion(output.view(-1, vocab_size), train_answer_sequences_output.view(-1)) # ask</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(output.reshape(<span class="op">-</span><span class="dv">1</span>, vocab_size), train_answer_sequences_output.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>time.time() <span class="op">-</span> init</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch [1/50], Loss: 8.87558364868164
Epoch [2/50], Loss: 8.849912643432617
Epoch [3/50], Loss: 8.822985649108887
Epoch [4/50], Loss: 8.792208671569824
Epoch [5/50], Loss: 8.754260063171387
Epoch [6/50], Loss: 8.703964233398438
Epoch [7/50], Loss: 8.631867408752441
Epoch [8/50], Loss: 8.517773628234863
Epoch [9/50], Loss: 8.311432838439941
Epoch [10/50], Loss: 7.934554100036621
Epoch [11/50], Loss: 7.5272321701049805
Epoch [12/50], Loss: 7.2270894050598145
Epoch [13/50], Loss: 7.004649639129639
Epoch [14/50], Loss: 6.82792854309082
Epoch [15/50], Loss: 6.6866374015808105
Epoch [16/50], Loss: 6.579782009124756
Epoch [17/50], Loss: 6.508011341094971
Epoch [18/50], Loss: 6.469689846038818
Epoch [19/50], Loss: 6.459895610809326
Epoch [20/50], Loss: 6.468969345092773
Epoch [21/50], Loss: 6.484289169311523
Epoch [22/50], Loss: 6.495165824890137
Epoch [23/50], Loss: 6.496269226074219
Epoch [24/50], Loss: 6.487391948699951
Epoch [25/50], Loss: 6.471380233764648
Epoch [26/50], Loss: 6.452081203460693
Epoch [27/50], Loss: 6.432642936706543
Epoch [28/50], Loss: 6.4148335456848145
Epoch [29/50], Loss: 6.399575710296631
Epoch [30/50], Loss: 6.3866286277771
Epoch [31/50], Loss: 6.375542163848877
Epoch [32/50], Loss: 6.36587381362915
Epoch [33/50], Loss: 6.35689115524292
Epoch [34/50], Loss: 6.3482255935668945
Epoch [35/50], Loss: 6.339582920074463
Epoch [36/50], Loss: 6.3308844566345215
Epoch [37/50], Loss: 6.321742534637451
Epoch [38/50], Loss: 6.312087059020996
Epoch [39/50], Loss: 6.30206298828125
Epoch [40/50], Loss: 6.291914463043213
Epoch [41/50], Loss: 6.281908988952637
Epoch [42/50], Loss: 6.272091865539551
Epoch [43/50], Loss: 6.262415885925293
Epoch [44/50], Loss: 6.2526750564575195
Epoch [45/50], Loss: 6.242735862731934
Epoch [46/50], Loss: 6.2325520515441895
Epoch [47/50], Loss: 6.222198486328125
Epoch [48/50], Loss: 6.211533546447754
Epoch [49/50], Loss: 6.200573921203613
Epoch [50/50], Loss: 6.1894450187683105
</code></pre>
</div>
<div class="output execute_result" data-execution_count="18">
<pre><code>466.6952950954437</code></pre>
</div>
</div>
<div id="9c5bad38" class="cell code" id="9c5bad38">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>torch.save(model, <span class="st">&#39;lstm_lm_model.pth&#39;</span>)</span></code></pre></div>
</div>
<div id="1d1c1613" class="cell code" id="1d1c1613"
data-outputId="b0eac940-c952-46b8-c05b-325b466e979b">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.load(<span class="st">&#39;lstm_lm_model.pth&#39;</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span></code></pre></div>
<div class="output execute_result" data-execution_count="21">
<pre><code>Seq2Seq(
  (embedding): Embedding(7095, 128)
  (encoder_lstm): LSTM(128, 256, batch_first=True)
  (decoder_lstm): LSTM(128, 256, batch_first=True)
  (fc): Linear(in_features=256, out_features=7095, bias=True)
)</code></pre>
</div>
</div>
<div id="630d4132" class="cell code" id="630d4132"
data-outputId="77eca62f-3071-4652-cefc-176af348a39c">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Validation</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    val_output, _ <span class="op">=</span> model(val_prompt_sequences, val_answer_sequences_input, <span class="va">None</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     val_loss = criterion(val_output.view(-1, vocab_size), val_answer_sequences_output.view(-1))</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> criterion(val_output.reshape(<span class="op">-</span><span class="dv">1</span>, vocab_size), val_answer_sequences_output.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Validation Loss: </span><span class="sc">{</span>val_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Validation Loss: 6.111302852630615
</code></pre>
</div>
</div>
<div id="d0f5c10b" class="cell code" id="d0f5c10b">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_simple_lm_response(prompt, model, vocab, max_len_answer):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        prompt_sequence <span class="op">=</span> pad_sequence([torch.tensor(text_pipeline(prompt))[:max_len_prompt]], batch_first<span class="op">=</span><span class="va">True</span>, padding_value<span class="op">=</span>vocab[<span class="st">&quot;&lt;unk&gt;&quot;</span>]).to(device)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        decoder_input <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>).to(device)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">None</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        decoded_sentence <span class="op">=</span> []</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_len_answer):</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>            output, hidden <span class="op">=</span> model(prompt_sequence, decoder_input, hidden)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>            sampled_token_index <span class="op">=</span> output.argmax(<span class="dv">2</span>)[:, <span class="op">-</span><span class="dv">1</span>].item()</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>            sampled_word <span class="op">=</span> vocab.lookup_token(sampled_token_index)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>            decoded_sentence.append(sampled_word)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> sampled_word <span class="op">==</span> <span class="st">&#39;&lt;end&gt;&#39;</span>:</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>            decoder_input <span class="op">=</span> torch.cat([decoder_input, torch.tensor([[sampled_token_index]], dtype<span class="op">=</span>torch.<span class="bu">long</span>).to(device)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(decoded_sentence)</span></code></pre></div>
</div>
<div id="787425df" class="cell code" id="787425df"
data-outputId="cb0ee39c-cd54-4b53-fa0f-7bc81cb9cd18">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate an answer - simple LM</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_pd[<span class="st">&#39;prompt&#39;</span>][<span class="dv">9</span>]</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prompt_)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_simple_lm_response(prompt_, model, vocab, max_len_answer))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Please give us creative product ideas related to air fresheners.
, and , and , and , and , and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the
</code></pre>
</div>
</div>
<div id="e5d487ed" class="cell raw" id="e5d487ed">

</div>
<div id="375bcfa4" class="cell code"
data-colab="{&quot;referenced_widgets&quot;:[&quot;85e4e1fe53154b9e880f43861f0accf6&quot;,&quot;81910856e7964587a716fc2b4b6d2708&quot;,&quot;9987e82ce73a4811a17b784163932a8b&quot;,&quot;afc20eccec8e45ef8c1267db4a5bcb97&quot;,&quot;02e210d097f44b9f82fab90dc5c1f37c&quot;,&quot;8bbee5f234f8407fa6cb47b3f19499db&quot;,&quot;0742746fe6274962b5f0244fa8ee2dc7&quot;,&quot;4a5c7ee39236454a9b735f258669e20a&quot;,&quot;8ac010741448483fb5991295b9533daf&quot;,&quot;53672949fecc455899fd961f3b711f0a&quot;,&quot;155ea8721fb64262982addf313ca7098&quot;,&quot;7729e598544f40bdbe83e93e7d2c3666&quot;]}"
id="375bcfa4" data-outputId="0b974c41-6afc-4770-8be1-c7f2d1b587cb">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>llm_model_name <span class="op">=</span> <span class="st">&quot;meta-llama/Meta-Llama-3-8B&quot;</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>llm_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(llm_model_name, add_bos_token<span class="op">=</span><span class="va">False</span>, add_eos_token<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>llm_tokenizer.pad_token <span class="op">=</span> llm_tokenizer.eos_token</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>llm_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(llm_model_name)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">&#39;cpu&#39;</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>llm_model.to(device)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb28"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;85e4e1fe53154b9e880f43861f0accf6&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb29"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;81910856e7964587a716fc2b4b6d2708&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb30"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;9987e82ce73a4811a17b784163932a8b&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb31"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;afc20eccec8e45ef8c1267db4a5bcb97&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb32"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;02e210d097f44b9f82fab90dc5c1f37c&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb33"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;8bbee5f234f8407fa6cb47b3f19499db&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb34"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;0742746fe6274962b5f0244fa8ee2dc7&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb35"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;4a5c7ee39236454a9b735f258669e20a&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb36"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;8ac010741448483fb5991295b9533daf&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb37"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;53672949fecc455899fd961f3b711f0a&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb38"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;155ea8721fb64262982addf313ca7098&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb39"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;7729e598544f40bdbe83e93e7d2c3666&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>cpu
</code></pre>
</div>
<div class="output execute_result" data-execution_count="9">
<pre><code>LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)</code></pre>
</div>
</div>
<div id="ff0a1eb9" class="cell code" id="ff0a1eb9"
data-outputId="b525d111-cf0b-4013-d745-398615c95e68">
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_llm_responses(text_):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> llm_tokenizer.encode_plus(text_, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>).to(device)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> inputs[<span class="st">&#39;input_ids&#39;</span>]</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    attention_mask <span class="op">=</span> inputs[<span class="st">&#39;attention_mask&#39;</span>]</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> llm_model.generate(input_ids, attention_mask<span class="op">=</span>attention_mask, max_length<span class="op">=</span><span class="dv">140</span>,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>                                 num_return_sequences<span class="op">=</span><span class="dv">1</span>, pad_token_id<span class="op">=</span>llm_tokenizer.eos_token_id)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> llm_tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_pd[<span class="st">&#39;prompt&#39;</span>][<span class="dv">9</span>]</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;meta lamma tokeniser output: &#39;</span>, llm_tokenizer.tokenize(prompt_))</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please give us creative product ideas related to air fresheners.
meta lamma tokeniser output:  [&#39;Please&#39;, &#39;give&#39;, &#39;us&#39;, &#39;creative&#39;, &#39;product&#39;, &#39;ideas&#39;, &#39;related&#39;, &#39;to&#39;, &#39;air&#39;, &#39;fresh&#39;, &#39;eners&#39;, &#39;.&#39;]
generated response:  Please give us creative product ideas related to air fresheners. I am looking for unique, new and innovative ideas for air fresheners. These ideas should be innovative and creative.
</code></pre>
</div>
</div>
<div id="28a72e0e" class="cell raw" id="28a72e0e">

</div>
<div id="d873952a" class="cell markdown" id="d873952a">
<h2
id="3-explain-and-implement-how-specific-tokenizer-and-model-can-work-well-with-the-mentioned-dataset-1-mark">3.
Explain and implement how specific tokenizer and model can work well
with the mentioned dataset. [1 Mark]</h2>
</div>
<div id="7ae8750f" class="cell raw" id="7ae8750f">

</div>
<div id="efdabdad" class="cell code"
data-colab="{&quot;referenced_widgets&quot;:[&quot;35fe169d45364eca95d5b4c378592ec3&quot;,&quot;b278c46a5a304331be40e70cffdaca20&quot;,&quot;9b73725720a6434495c6da363fc64f30&quot;,&quot;c62600a008844055b147594bde7c75ca&quot;,&quot;58759c35d8874ad2a46c965b84fb1677&quot;]}"
id="efdabdad" data-outputId="edc67f9c-6133-40ac-d00f-941518da9087">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>bpe_tokenizer <span class="op">=</span> RobertaTokenizer.from_pretrained(<span class="st">&quot;roberta-base&quot;</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>bpe_tokenized_prompt_ <span class="op">=</span> bpe_tokenizer.tokenize(prompt_)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;BPE Tokenized Prompts: &quot;</span>, bpe_tokenized_prompt_)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb45"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;35fe169d45364eca95d5b4c378592ec3&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb46"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;b278c46a5a304331be40e70cffdaca20&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb47"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;9b73725720a6434495c6da363fc64f30&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb48"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;c62600a008844055b147594bde7c75ca&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb49"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;58759c35d8874ad2a46c965b84fb1677&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>BPE Tokenized Prompts:  [&#39;Please&#39;, &#39;give&#39;, &#39;us&#39;, &#39;creative&#39;, &#39;product&#39;, &#39;ideas&#39;, &#39;related&#39;, &#39;to&#39;, &#39;air&#39;, &#39;fres&#39;, &#39;hen&#39;, &#39;ers&#39;, &#39;.&#39;]
</code></pre>
</div>
</div>
<div id="613e8cae" class="cell code" id="613e8cae"
data-outputId="88e3615b-87b3-4dfa-afa8-b335718f12da">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>char_tokenizer <span class="op">=</span> Tokenizer(models.WordLevel(unk_token<span class="op">=</span><span class="st">&quot;[UNK]&quot;</span>))</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>char_tokenizer.pre_tokenizer <span class="op">=</span> pre_tokenizers.Whitespace()</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>char_tokenizer.decoder <span class="op">=</span> decoders.WordPiece(prefix<span class="op">=</span><span class="st">&quot;##&quot;</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>vocab_size_<span class="op">=</span>vocab_size</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the tokenizer on the dataset</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> trainers.WordLevelTrainer(vocab_size<span class="op">=</span>vocab_size_, special_tokens<span class="op">=</span>[<span class="st">&quot;[UNK]&quot;</span>, <span class="st">&quot;[PAD]&quot;</span>, <span class="st">&quot;[CLS]&quot;</span>, <span class="st">&quot;[SEP]&quot;</span>, <span class="st">&quot;[MASK]&quot;</span>])</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>char_tokenizer.train_from_iterator(train_df[<span class="st">&#39;prompt&#39;</span>], trainer)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize using the character-level tokenizer</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>char_tokenized_prompt_ <span class="op">=</span> char_tokenizer.encode(prompt).tokens</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Character-Level Tokenized Prompts: &quot;</span>, char_tokenized_prompt_)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Character-Level Tokenized Prompts:  [&#39;explain&#39;, &#39;master&#39;, &#39;slave&#39;, &#39;replication&#39;, &#39;[UNK]&#39;]
</code></pre>
</div>
</div>
<div id="635a856c" class="cell code" id="635a856c"
data-outputId="ff5f4fda-2ce5-4f58-822c-e21842974c79">
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate predictions using BPE tokenizer</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> bpe_tokenizer.encode_plus(prompt_, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> inputs[<span class="st">&#39;input_ids&#39;</span>].to(device)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>attention_mask <span class="op">=</span> inputs[<span class="st">&#39;attention_mask&#39;</span>].to(device)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm_model.generate(input_ids, attention_mask<span class="op">=</span>attention_mask, max_length<span class="op">=</span><span class="dv">140</span>,</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>                             num_return_sequences<span class="op">=</span><span class="dv">1</span>, pad_token_id<span class="op">=</span>bpe_tokenizer.eos_token_id)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> bpe_tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;\n&#39;)</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;bpe encoded response: &#39;</span>, response)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please give us creative product ideas related to air fresheners.
bpe encoded response:  Please give us creative product ideas related to air fresheners. next&#39;s for is for for us. next&#39;s for is for for&#39;s us. next&#39;s for is for for with us. next&#39;s for is for for The us. next&#39;s for is for for was us. next&#39;s for is for for &quot; us. next&#39;s for is for for at us. next&#39;s for is for for it us. next&#39;s for is for forw us. next&#39;s for is for for technology us. next&#39;s for is for forE us. next&#39;s for is
</code></pre>
</div>
</div>
<div id="4717fabb" class="cell code" id="4717fabb">
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Generate predictions using BPE tokenizer</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co"># inputs = bpe_tokenizer.encode_plus(prompt_, return_tensors=&#39;pt&#39;, padding=True, truncation=True)</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co"># input_ids = inputs[&#39;input_ids&#39;]</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># attention_mask = inputs[&#39;attention_mask&#39;]</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="co">#                              num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;prompt: &#39;, prompt_)</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;\n&#39;)</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;bpe encoded response: &#39;, response)</span></span></code></pre></div>
</div>
<div id="46b65d2c" class="cell code" id="46b65d2c"
data-outputId="06cec77a-2fd8-4be9-dfe8-bb7dec56253a">
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate predictions using char tokenizer</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>encoding <span class="op">=</span> char_tokenizer.encode(prompt_)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([encoding.ids], dtype<span class="op">=</span>torch.<span class="bu">long</span>).to(device)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>attention_mask <span class="op">=</span> torch.ones_like(input_ids).to(device)  <span class="co"># Dummy attention mask, since char_tokenizer doesn&#39;t provide it</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm_model.generate(input_ids, attention_mask<span class="op">=</span>attention_mask, max_length<span class="op">=</span><span class="dv">140</span>,</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>                             num_return_sequences<span class="op">=</span><span class="dv">1</span>, pad_token_id<span class="op">=</span>llm_tokenizer.eos_token_id)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> char_tokenizer.decode(outputs[<span class="dv">0</span>].cpu().numpy().tolist())</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;char level encoded response: &#39;</span>, response)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please give us creative product ideas related to air fresheners.


char level encoded response:  Please give us creative product ideas related to air. to. like. to. like..........................................................
</code></pre>
</div>
</div>
<div id="d1fce81e" class="cell raw" id="d1fce81e">

</div>
<div id="e81cb062" class="cell markdown" id="e81cb062">
<h2
id="4-explain-and-implement-how-sentiment-analysis-can-be-used-to-analyze-user-questions-or-contexts-1-mark">4.
Explain and implement how sentiment analysis can be used to analyze user
questions or contexts. [1 Mark]</h2>
</div>
<div id="02586b03" class="cell code" id="02586b03">
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the sentiment analysis pipeline</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>sentiment_pipeline <span class="op">=</span> pipeline(<span class="st">&quot;sentiment-analysis&quot;</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="co"># A list of example questions is created to demonstrate how sentiment analysis works.</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>user_texts <span class="op">=</span> [</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;I&#39;m really happy with the new product!&quot;</span>,</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;I had a terrible experience with the customer service.&quot;</span>,</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Can you help me with my issue?&quot;</span>,</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;The latest update is okay, but it could be better.&quot;</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Each question is analyzed for sentiment, and the results are printed out.</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> user_texts:</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>    sentiment <span class="op">=</span> sentiment_pipeline(text) <span class="co"># It takes a text input, runs it through the sentiment analysis pipeline, and returns the result.</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Sentiment: </span><span class="sc">{</span>sentiment<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code></pre></div>
</div>
<div id="861b7ce3" class="cell raw" id="861b7ce3">

</div>
<div id="b9eff837" class="cell raw" id="b9eff837">

</div>
<div id="59cd904b" class="cell raw" id="59cd904b">

</div>
<div id="83ba8c35" class="cell markdown" id="83ba8c35">
<h2
id="5-implement-fine-tuning-for-an-open-source-model-to-improve-the-results-3-marks">5.
Implement fine-tuning for an open-source model to improve the results.
[3 Marks]</h2>
</div>
<div id="e0fa7119" class="cell code"
data-colab="{&quot;referenced_widgets&quot;:[&quot;9f9e7bb3a168444f800e230de97af181&quot;,&quot;6aee2e6b42fe4a96b5b5e10bdbb4e488&quot;]}"
id="e0fa7119" data-outputId="1f082f27-623a-45f0-d3e6-8e3266d0c988">
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> llm_tokenizer(examples[<span class="st">&#39;prompt&#39;</span>], add_special_tokens<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>, padding<span class="op">=</span><span class="st">&quot;max_length&quot;</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> llm_tokenizer(examples[<span class="st">&#39;response&#39;</span>], add_special_tokens<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>, padding<span class="op">=</span><span class="st">&quot;max_length&quot;</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    inputs[<span class="st">&#39;labels&#39;</span>] <span class="op">=</span> outputs[<span class="st">&#39;input_ids&#39;</span>]</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> inputs</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">&quot;nvidia/HELPSTEER2&quot;</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>dataset[<span class="st">&quot;train&quot;</span>].column_names)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>tokenized_datasets.set_format(<span class="st">&quot;torch&quot;</span>)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> tokenized_datasets[<span class="st">&quot;train&quot;</span>].select(<span class="bu">range</span>(<span class="dv">20</span>))</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>eval_dataset <span class="op">=</span> tokenized_datasets[<span class="st">&quot;validation&quot;</span>].select(<span class="bu">range</span>(<span class="dv">20</span>))</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb60"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;9f9e7bb3a168444f800e230de97af181&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb61"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;6aee2e6b42fe4a96b5b5e10bdbb4e488&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div id="fed32a75" class="cell code" id="fed32a75">
<div class="sourceCode" id="cb62"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data collator</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForLanguageModeling(tokenizer<span class="op">=</span>llm_tokenizer, mlm<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div id="31dfecfc" class="cell code" id="31dfecfc"
data-outputId="dab4baac-fdcc-43e4-a9b5-8e89553046d3"
data-scrolled="false">
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>train_dataset</span></code></pre></div>
<div class="output execute_result" data-execution_count="56">
<pre><code>Dataset({
    features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;],
    num_rows: 20
})</code></pre>
</div>
</div>
<div id="49880c7b" class="cell code" id="49880c7b">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">&quot;./results&quot;</span>,</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">&quot;epoch&quot;</span>,</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    use_cpu<span class="op">=</span><span class="va">True</span>,  <span class="co"># Ensures training is on CPU</span></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="806c3a72" class="cell code" id="806c3a72"
data-outputId="5d859759-5a97-4660-f017-879b5a03b87f"
data-scrolled="false">
<div class="sourceCode" id="cb66"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>train_dataset</span></code></pre></div>
<div class="output execute_result" data-execution_count="56">
<pre><code>Dataset({
    features: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;],
    num_rows: 20
})</code></pre>
</div>
</div>
<div id="41fd8bed" class="cell code" id="41fd8bed"
data-outputId="75cd4c09-3b4d-4a8f-dd2d-f9c4250b41ac">
<div class="sourceCode" id="cb68"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> time.time()</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Trainer</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>llm_model,</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>eval_dataset,</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>time.time()<span class="op">-</span>init</span></code></pre></div>
<div class="output display_data">

    <div>
      
      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [9/9 13:25, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>5.521920</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>4.785708</td>
    </tr>
    <tr>
      <td>3</td>
      <td>No log</td>
      <td>4.745604</td>
    </tr>
  </tbody>
</table><p>
</div>
<div class="output stream stderr">
<pre><code>We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="58">
<pre><code>889.6026239395142</code></pre>
</div>
</div>
<div id="9541f618" class="cell code" id="9541f618"
data-outputId="2ab09422-de1a-4dc9-b8e0-5d14e8a801af">
<div class="sourceCode" id="cb71"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> time.time()</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="st">&quot;sft_finetuned_llm_model.pth&quot;</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>torch.save(llm_model.state_dict(), model_save_path)</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Model saved to </span><span class="sc">{</span>model_save_path<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>time.time() <span class="op">-</span> init</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model saved to sft_finetuned_llm_model.pth
</code></pre>
</div>
</div>
<div id="e5f47237" class="cell markdown" id="e5f47237">
<h2
id="6-show-5-10-working-examples-that-show-improvements-of-the-accuracy-of-the-fine-tuned-model-3-marks">6.
Show 5-10 working examples that show improvements of the accuracy of the
fine-tuned model. [3 Marks]</h2>
</div>
<div id="592c151d" class="cell code" id="592c151d">
<div class="sourceCode" id="cb73"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># loading original model and finding responses on old and sft trained model on 6 prompts in val df</span></span></code></pre></div>
</div>
<div id="bdde17c6" class="cell code"
data-colab="{&quot;referenced_widgets&quot;:[&quot;c9016f5cf3e64a27a66d0ab39202dbdc&quot;]}"
id="bdde17c6" data-outputId="79434c5b-45d3-4cb6-84c9-34f8102b4ee9">
<div class="sourceCode" id="cb74"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>m1_tokeniser <span class="op">=</span> AutoTokenizer.from_pretrained(llm_model_name, add_bos_token<span class="op">=</span><span class="va">False</span>, add_eos_token<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>m1_tokeniser.pad_token <span class="op">=</span> m1_tokeniser.eos_token</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>m1_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(llm_model_name)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb75"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;c9016f5cf3e64a27a66d0ab39202dbdc&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div id="799db096" class="cell code" id="799db096">
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prompt 1</span></span></code></pre></div>
</div>
<div id="0cd84071" class="cell code" id="0cd84071"
data-outputId="b22d1661-f2de-457a-d485-f2e3c4b5e3e1">
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_llm_responses(text_):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> llm_tokenizer.encode_plus(text_, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>).to(device)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> inputs[<span class="st">&#39;input_ids&#39;</span>]</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    attention_mask <span class="op">=</span> inputs[<span class="st">&#39;attention_mask&#39;</span>]</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> llm_model.generate(input_ids, attention_mask<span class="op">=</span>attention_mask, max_length<span class="op">=</span><span class="dv">140</span>,</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>                                 num_return_sequences<span class="op">=</span><span class="dv">1</span>, pad_token_id<span class="op">=</span>llm_tokenizer.eos_token_id)</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> llm_tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> <span class="st">&#39;Please give us creative product ideas related to air fresheners.&#39;</span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;meta lamma tokeniser output: &#39;</span>, llm_tokenizer.tokenize(prompt_))</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please give us creative product ideas related to air fresheners.
meta lamma tokeniser output:  [&#39;Please&#39;, &#39;give&#39;, &#39;us&#39;, &#39;creative&#39;, &#39;product&#39;, &#39;ideas&#39;, &#39;related&#39;, &#39;to&#39;, &#39;air&#39;, &#39;fresh&#39;, &#39;eners&#39;, &#39;.&#39;]
generated response:  Please give us creative product ideas related to air fresheners.
</code></pre>
</div>
</div>
<div id="975a7922" class="cell code" id="975a7922"
data-outputId="169254b5-4eac-413d-97c6-b8a7da20a76a">
<div class="sourceCode" id="cb79"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_m1_responses(text_):</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span>m1_tokeniser.encode_plus(text_, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>).to(device)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> inputs[<span class="st">&#39;input_ids&#39;</span>]</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    attention_mask <span class="op">=</span> inputs[<span class="st">&#39;attention_mask&#39;</span>]</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> m1_model.generate(input_ids, attention_mask<span class="op">=</span>attention_mask, max_length<span class="op">=</span><span class="dv">140</span>,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>                                 num_return_sequences<span class="op">=</span><span class="dv">1</span>, pad_token_id<span class="op">=</span>m1_tokeniser.eos_token_id)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> m1_tokeniser.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> <span class="st">&#39;Please give us creative product ideas related to air fresheners.&#39;</span></span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;meta lamma tokeniser output: &#39;</span>, m1_tokeniser.tokenize(prompt_))</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_m1_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please give us creative product ideas related to air fresheners.
meta lamma tokeniser output:  [&#39;Please&#39;, &#39;give&#39;, &#39;us&#39;, &#39;creative&#39;, &#39;product&#39;, &#39;ideas&#39;, &#39;related&#39;, &#39;to&#39;, &#39;air&#39;, &#39;fresh&#39;, &#39;eners&#39;, &#39;.&#39;]
generated response:  Please give us creative product ideas related to air fresheners. We are looking for creative ideas that can be turned into a product.
I&#39;m looking for an air freshener with a unique scent. I need it to be something that I can sell to people who have dogs and need to freshen their homes.
I am looking for a creative product idea related to air fresheners. I need it to be something that I can sell to people who are looking for a unique way to freshen their homes.
I&#39;m looking for a creative product idea related to air fresheners. I need it to be something that I can sell to people who are looking for a unique way to freshen their homes
</code></pre>
</div>
</div>
<div id="8306faf7" class="cell code" id="8306faf7">
<div class="sourceCode" id="cb81"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prompt 2</span></span></code></pre></div>
</div>
<div id="36a98eaf" class="cell code" id="36a98eaf"
data-outputId="2cb991d8-846c-49fa-a5bc-6216d95bfeea">
<div class="sourceCode" id="cb82"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">14</span>]</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  what is SoP for studying abroad students

generated response:  what is SoP for studying abroad students

</code></pre>
</div>
</div>
<div id="48a02937" class="cell code" id="48a02937"
data-outputId="ae9cc5aa-602a-4d5d-d76d-6f9db73e1286">
<div class="sourceCode" id="cb84"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">14</span>]</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, m1_tokeniser.tokenize(prompt_))</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_m1_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  what is SoP for studying abroad students

generated response:  what is SoP for studying abroad students
The SoP for studying abroad students is a statement of purpose that helps you to get admission to a university. It is a document that you write to explain why you want to study abroad and what you hope to achieve by doing so. It should be written in a clear and concise manner, and it should be tailored to the specific university you are applying to. The SoP for studying abroad students is an important part of the application process, and it can help you to stand out from other applicants.
What is a SoP for studying abroad students?
A SoP for studying abroad students is a statement of purpose that helps students to explain their motivation for
</code></pre>
</div>
</div>
<div id="d98c05a9" class="cell code" id="d98c05a9">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prompt 3</span></span></code></pre></div>
</div>
<div id="81087694" class="cell code" id="&quot;81087694&quot;"
data-outputId="1edbf893-6f73-493a-f2f7-ec5ab185c8b9">
<div class="sourceCode" id="cb87"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">18</span>]</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns
generated response:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns
</code></pre>
</div>
</div>
<div id="47d2d40c" class="cell code" id="47d2d40c"
data-outputId="b4ffbbe2-5388-43c4-f145-bf5bc25e5b2d">
<div class="sourceCode" id="cb89"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">18</span>]</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, m1_tokeniser.tokenize(prompt_))</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_m1_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns
generated response:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns. She is standing in front of a dark, ominous, tree.
I think you need to give more context. Is she a demon? A witch? A vampire? A ghost? A zombie? A goth? A succubus? A demoness? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus?
</code></pre>
</div>
</div>
<div id="b4104784" class="cell code" id="b4104784">
<div class="sourceCode" id="cb91"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prompt 4</span></span></code></pre></div>
</div>
<div id="992fa8f8" class="cell code" id="992fa8f8"
data-outputId="a3d83c80-a877-4b4f-9b06-5aa24ab0172e">
<div class="sourceCode" id="cb92"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">24</span>]</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please provide the equation to calculate the moment of inertia of a trapezium
generated response:  Please provide the equation to calculate the moment of inertia of a trapezium
</code></pre>
</div>
</div>
<div id="fc4dc1d9" class="cell code" id="fc4dc1d9"
data-outputId="d02dd8b8-0439-4ae5-8712-2147a5327580">
<div class="sourceCode" id="cb94"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">24</span>]</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_m1_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Please provide the equation to calculate the moment of inertia of a trapezium
generated response:  Please provide the equation to calculate the moment of inertia of a trapezium about the centroidal axis.
The moment of inertia of a trapezium about the centroidal axis can be calculated using the formula:
I = \frac{bh^3}{12} + \frac{dh^3}{12}
I = moment of inertia of the trapezium
b = base length
h = height of the trapezium
d = distance from the centroidal axis to the base of the trapezium
</code></pre>
</div>
</div>
<div id="e79bcbf7" class="cell code" id="e79bcbf7">
<div class="sourceCode" id="cb96"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prompt 5</span></span></code></pre></div>
</div>
<div id="dd008b4d" class="cell code" id="dd008b4d"
data-outputId="b0750956-9351-474c-b2cd-b000ed583fc1">
<div class="sourceCode" id="cb97"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">60</span>]</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  What are the different components of a business plan
generated response:  What are the different components of a business plan
</code></pre>
</div>
</div>
<div id="614d0100" class="cell code" id="614d0100"
data-outputId="c45cdc41-1294-483e-fa87-fd8a9c71806a">
<div class="sourceCode" id="cb99"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">60</span>]</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_m1_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  What are the different components of a business plan
generated response:  What are the different components of a business plan?
What are the different components of a business plan?
The key components of a business plan include:
What are the 4 elements of a business plan?
The four key elements of a business plan are:
The Executive Summary. The executive summary is the first section of the business plan.
The Company Description.
The Market Analysis.
The Marketing Plan.
The Company Organization.
The Service or Product Line.
The Marketing Strategy.
The Competition Analysis.
What is the most important part of a business plan?
The executive summary is the most important part of your business plan. It is the first thing that potential investors will see, and it will determine whether they will read
</code></pre>
</div>
</div>
<div id="fd5400b8" class="cell code" id="fd5400b8">
<div class="sourceCode" id="cb101"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prompt 6</span></span></code></pre></div>
</div>
<div id="25e80d1f" class="cell code" id="25e80d1f"
data-outputId="03c51f97-1005-41ee-ab12-99260e981a01">
<div class="sourceCode" id="cb102"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">110</span>]</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_llm_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Explain Business plan legal use vs operating agreement
generated response:  Explain Business plan legal use vs operating agreement
</code></pre>
</div>
</div>
<div id="a681f8b6" class="cell code" id="a681f8b6"
data-outputId="e3fa2e15-4072-47aa-fd38-8c3faf7911e2">
<div class="sourceCode" id="cb104"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>prompt_ <span class="op">=</span> val_df[<span class="st">&#39;prompt&#39;</span>][<span class="dv">110</span>]</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;prompt: &#39;</span>, prompt_)</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(&#39;meta lamma tokeniser output: &#39;, llm_tokenizer.tokenize(prompt_))</span></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;generated response: &#39;</span>, generate_m1_responses(prompt_))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>prompt:  Explain Business plan legal use vs operating agreement
generated response:  Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
Explain Business plan legal use vs operating agreement
</code></pre>
</div>
</div>
<div id="098187f8" class="cell raw" id="098187f8">

</div>
<div id="e45db978" class="cell markdown" id="e45db978">
<h2
id="7-apply-and-fine-tune-the-generator-model-using-parameter-efficient-fine-tuning-peft-3-marks">7.
Apply and fine-tune the generator model using Parameter-Efficient
Fine-Tuning (PEFT). [3 Marks]</h2>
</div>
<div id="EA0EOUb7n3zr" class="cell code" data-execution_count="6"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:641,&quot;referenced_widgets&quot;:[&quot;6da503d2443a49308165f17cd99a15c9&quot;,&quot;ff7f90c5efd84165a8168fca2228744c&quot;,&quot;0ebafc1e122b4f9dac6545a90fd47b42&quot;,&quot;5a0ed4b96c214b1b8e1a9ed7c1daea4c&quot;,&quot;d2556b16305f46d8a8a59c3ab4254245&quot;,&quot;d69f690cc6cb4b24bbcb252bf88e372d&quot;,&quot;d7ad1bc6f43b4a7a9da28306850137cd&quot;,&quot;51a423e1b2c04150b6f45fe966f4457a&quot;,&quot;9e488e7982c74dfea25322996dac1474&quot;,&quot;673ba88e71364140afeff879c940782e&quot;,&quot;530a8fbe2ff740eca5981724981e4068&quot;,&quot;5e32a307ca4644849cdb1e0bf41762b3&quot;,&quot;8672564d4add421e96b905b6e7772f5b&quot;,&quot;248ccbd0e29941de96d0a983d78b8e47&quot;,&quot;46b20ae4559642fab43224c41381b1cc&quot;,&quot;693087b2d6fe4cc489c0610cefe3fea0&quot;,&quot;bdf2245e25684bbdb07a374573a9f777&quot;,&quot;0f5469f168c44070b0bf3232f11f3772&quot;,&quot;841788582d124b5299d828c5dc0e29ac&quot;,&quot;9870a91b85644748b4aa6ee0b664f6e2&quot;,&quot;75fe6c12bbfe43b49d9f8c1cfee21079&quot;,&quot;fdcbd15ee35844349e28a940bf7c8a32&quot;,&quot;0a07be9678a944819523edea4c37450e&quot;,&quot;bb4dcef7497a414c934b0b83977652e0&quot;,&quot;7d3231ce29554d62af682d8a223258e9&quot;,&quot;fdbfa0ab06694161a0fcab202c77ff73&quot;,&quot;eb458da09c7c48b29f1e045dbbf8c3ed&quot;,&quot;2187d9c66df041c691a1b4feb979e648&quot;,&quot;ead04f7ea8a246a79175846b4c9e0d4b&quot;,&quot;0799154438dc4d51a2df0731e78660c0&quot;,&quot;97c924a762d64d55b3f28eb46bcb98db&quot;,&quot;b590647f630643d4b6afd51c43fd38e4&quot;,&quot;100e0177cbd1449795e540bf949410a6&quot;,&quot;ed8e9e28b92943fd80d910f775bdec95&quot;,&quot;509a48ff360641f5b928f0a5b618ba06&quot;,&quot;d9999677468e481f99b5843f96acdb5d&quot;,&quot;39305cf1de6a418e81b2bdbe10de3fa5&quot;,&quot;0b41b7e35f9c402caa64f5a38318b2ce&quot;,&quot;e97eb83c7f8b46e680f8a64b61dd1786&quot;,&quot;e4b091535f4c45688a0fc18563b7fbaf&quot;,&quot;ac134b865a0b4ef49de7ceb4d4c965ec&quot;,&quot;1b736f587eed4d9eb2761d16f7d6a16d&quot;,&quot;bad509de23a64f2a93735fcaf41e4c80&quot;,&quot;c7cf41b5d4a743d8952849cd063e165a&quot;,&quot;8fdf17c607df494486d0a7ec781bc50a&quot;,&quot;5a80894cd9e843d88b6a8455bc699e94&quot;,&quot;419cb4f3081943ea8ececb8231658529&quot;,&quot;b21d3ee1d3854bb495ce7a04e7e645e1&quot;,&quot;b803dc8b16b049cd82513605cfef6ab5&quot;,&quot;77e7b49747d24638a60e6d6a45ad721b&quot;,&quot;eab31031a99b418a9c442a75e734237f&quot;,&quot;648f13c4e72143778cee2ad772a4126e&quot;,&quot;fdc5ef5c63ce41c3a82f6ff39bf64968&quot;,&quot;3f086d11f62b484bb2295139ac1a9fa6&quot;,&quot;96cb210d0e5e47f9a19b197a90c1c358&quot;,&quot;3c327f4e3a904634a739494db8e403f0&quot;,&quot;01e0dc59594f40659e381f2265eebb93&quot;,&quot;9ef453f95a104c759a085ab644d01946&quot;,&quot;7f4416c7a1554ac89585b768589b10b4&quot;,&quot;ed81589f89ec41a5a2be49cf89d776b5&quot;,&quot;62370613cb9641b5baa0c3c0131bdc50&quot;,&quot;86397e1cc6724f5092f09a3f9cfbe10d&quot;,&quot;6696803876e74390bfad129aab4dd582&quot;,&quot;e31e345ea8c442c5b3f242670cfcf38a&quot;,&quot;8168ccc62d424f3e805c5304c7a27a1e&quot;,&quot;a8a25061703843d8b6e7d2f8d22154da&quot;,&quot;cf248685098149da8c7759d4ce865371&quot;,&quot;a8ddf9a5ad334c428c61a18a51d5a611&quot;,&quot;c75b6d4b91f44249981fc874e53716e4&quot;,&quot;f8c981976f3144d7bcf05a04417b24fa&quot;,&quot;3154ed1a9eeb4d9ab502ad6366347377&quot;,&quot;8ce4d4b4c592454dab28a368357c7b6b&quot;,&quot;1cf6bb6bbd564d519d45eecd0757b8c5&quot;,&quot;177ec30abe214e3ea60ed7a67d3ac393&quot;,&quot;16b0fe9697bf4850a7b0b87c1400808f&quot;,&quot;ff3f710b9cc54df09c2371111a0c4a0a&quot;,&quot;083110a91bd9484f9f46e9511311013a&quot;,&quot;f617519da9e94764afe2d9dee9dd452e&quot;,&quot;a7ff20aaf3a1491da766a9d5f7670a38&quot;,&quot;85c28aa836324b86bfb1aef21cf75f43&quot;,&quot;11cefb0aacbe4a14bab600efb4958397&quot;,&quot;e5924aa165a643c88b32d05ebe3bf558&quot;,&quot;a9691b1c8b2e465e85d218d1a70944d0&quot;,&quot;8a851a08c6b2498d924975c663e80f88&quot;,&quot;e10f413171d4465b9a3ffbac06400f0a&quot;,&quot;4e251a79be7f43708705a46b7cac14ff&quot;,&quot;9d85c4905c4a4b2a9282a4690da4e82d&quot;,&quot;d0df7068021f4271a4ea6e43b96a43ed&quot;,&quot;dc16c496a71f40008f6be48b1210f5f5&quot;,&quot;cc101b98f4c64d6a9085703efb1ceb6e&quot;,&quot;8919f116dd7b46ec897804cea51cfde9&quot;,&quot;9c44ff6b2ad8414c9a847cb731c56658&quot;,&quot;4cd01584b0e74f2a8395fc3511d36ae5&quot;,&quot;f89f856cfc87403490b549129edc134e&quot;,&quot;3dfa73b6c98d42c4bb8cc30cd331a25c&quot;,&quot;42449550904b4baa864c3d80db95b52f&quot;,&quot;c133df0373e44abbb4808aeb5073e9f8&quot;,&quot;b1bc0284eeb94b468cadfdb4477b5016&quot;,&quot;f83dd5cf4d024c2dba88042920646080&quot;,&quot;24fc828d8f2c48209064323fba7c979f&quot;,&quot;1bd60e3739a444aa8f2715fa76358d55&quot;,&quot;bf9bf978a34f414f9d40afd35a6610ba&quot;,&quot;e23028caa47e4afa9adbe730bafe95da&quot;,&quot;5468809b9156468b9be352ee9f997a73&quot;,&quot;ad9beb06c80a404e805fd5b43155aca9&quot;,&quot;2bc4573ca84347f6a3f46a47d103bf93&quot;,&quot;742d9f2a1fb64b928835e5c4a4287ced&quot;,&quot;816a323b634b46a495c111fcf9b6a746&quot;,&quot;4d5af18b253f47a7b37822798ff253b0&quot;,&quot;7afad3e2b09a4c1f833db50157b10780&quot;,&quot;e1a0ea7392eb4c3498ece72cb3337a93&quot;,&quot;a9bec6a1d52c4221834957441af95ae2&quot;,&quot;3177052daf354c7aa9d6c4d19101ccb9&quot;,&quot;83f90b2a7f74408b88863f723c20f9cd&quot;,&quot;6e9084604ca3463fa918e1761cfb6bf9&quot;,&quot;b807fcfaa2f4408598dbbdf6652c1e1f&quot;,&quot;0e529665ed6a4b0dbdc3f03187b377d4&quot;,&quot;403b4cf5017b47ee9db9ba8cf86bd69d&quot;,&quot;b70b57ebdc514e708799b19eb304d77d&quot;,&quot;7802507d236b45f59f2aa9faa0688c5d&quot;,&quot;809d46e82acd40639b47559079aa82df&quot;,&quot;ff03cbc374a440d988dba1992e4d138b&quot;,&quot;fb2133212ebe48db9754ad5f7a517883&quot;,&quot;fa5629ca8aa8439faa14acf9295e0011&quot;,&quot;0bebed7f28a24f2298cc1aeae3674407&quot;,&quot;56fdc87dc286463bbec92fd5ac1e2797&quot;,&quot;35f6ab91527a4d9e97c3895b2c525f51&quot;,&quot;c79f84391f3f47c18434b3424481608d&quot;,&quot;9e31d711236642e59e90be444d54ad4d&quot;,&quot;73f17a5b11974d67b9eb906f5260000c&quot;,&quot;40f6367935644682b0352b76cee93096&quot;,&quot;0fe79a64502d4c9ba0009dbc87192193&quot;,&quot;8f106423a25a4853b557c6db3b1f32de&quot;,&quot;c7cc46700ddc4d3f8ebcad426ee85643&quot;,&quot;4d42edf648d64a0bbb93f9636eef77a8&quot;,&quot;b1c454ed817d46f495b74f5833bc81e2&quot;,&quot;8ac35408b6414f269ccaf1fb4b04a7d1&quot;,&quot;7d59485824134da4a3e248164f58c868&quot;,&quot;7a7b3449b3b14c83af52ae0e976eac3b&quot;,&quot;00f4ff3428a741e6b2ba259569b04b0c&quot;,&quot;bae719fe6793488d9a13d991a39624cc&quot;,&quot;5c35b1a29d894dd49b61bf1a495ceb86&quot;,&quot;288eba27e719489fa111b5606b9f5b15&quot;,&quot;9f9d2b6067e5438da93fc0681aa1cb16&quot;,&quot;e886450c0ece4c7ea275a9fe1c94072b&quot;,&quot;58d7e99f0a32418f88d93b84e9750771&quot;,&quot;22ddb10ceb9f42a39480275e03442e55&quot;,&quot;77e52e684b2d42b2ab0048ae51150930&quot;,&quot;a56a530ce29d4b9bab1b303780dddf9f&quot;,&quot;e3620639d043469491e513b4052ab86b&quot;,&quot;a8c284e55fff4da5b82f0f300e726d5a&quot;,&quot;7496a52db6574e9ea102a3ced29ff124&quot;,&quot;da13b31c57b24e5e89caf00a13108abc&quot;,&quot;a4405cc07c5b467c8975b4aeea22544d&quot;]}"
id="EA0EOUb7n3zr" data-outputId="ec50f675-aec9-4c83-b19c-67e73ddac9ab">
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co">#imports</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">&quot;ignore&quot;</span>)</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">&quot;CUDA_VISIBLE_DEVICES&quot;</span>]<span class="op">=</span><span class="st">&quot;0&quot;</span></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> bitsandbytes <span class="im">as</span> bnb</span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoConfig, AutoModelForCausalLM</span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>torch.random.manual_seed(<span class="dv">0</span>)</span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a><span class="co">#load the base model.</span></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;microsoft/Phi-3-mini-128k-instruct&quot;</span>,</span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">&quot;cuda&quot;</span>,</span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span><span class="st">&quot;auto&quot;</span>,</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>    trust_remote_code<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a>     token<span class="op">=</span>token</span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">&quot;microsoft/Phi-3-mini-128k-instruct&quot;</span>, token<span class="op">=</span>token)</span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a><span class="co"># freezr the parameters.</span></span>
<span id="cb106-24"><a href="#cb106-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb106-25"><a href="#cb106-25" aria-hidden="true" tabindex="-1"></a>  param.requires_grad <span class="op">=</span> <span class="va">False</span>  <span class="co"># freeze the model - train adapters later</span></span>
<span id="cb106-26"><a href="#cb106-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> param.ndim <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb106-27"><a href="#cb106-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cast the small parameters (e.g. layernorm) to fp32 for stability</span></span>
<span id="cb106-28"><a href="#cb106-28" aria-hidden="true" tabindex="-1"></a>    param.data <span class="op">=</span> param.data.to(torch.float32)</span>
<span id="cb106-29"><a href="#cb106-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-30"><a href="#cb106-30" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()  <span class="co"># reduce number of stored activations</span></span>
<span id="cb106-31"><a href="#cb106-31" aria-hidden="true" tabindex="-1"></a>model.enable_input_require_grads()</span>
<span id="cb106-32"><a href="#cb106-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-33"><a href="#cb106-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CastOutputToFloat(nn.Sequential):</span>
<span id="cb106-34"><a href="#cb106-34" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x): <span class="cf">return</span> <span class="bu">super</span>().forward(x).to(torch.float32)</span>
<span id="cb106-35"><a href="#cb106-35" aria-hidden="true" tabindex="-1"></a>model.lm_head <span class="op">=</span> CastOutputToFloat(model.lm_head)</span>
<span id="cb106-36"><a href="#cb106-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-37"><a href="#cb106-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-38"><a href="#cb106-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_trainable_parameters(model):</span>
<span id="cb106-39"><a href="#cb106-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb106-40"><a href="#cb106-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Prints the number of trainable parameters in the model.</span></span>
<span id="cb106-41"><a href="#cb106-41" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb106-42"><a href="#cb106-42" aria-hidden="true" tabindex="-1"></a>    trainable_params <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb106-43"><a href="#cb106-43" aria-hidden="true" tabindex="-1"></a>    all_param <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb106-44"><a href="#cb106-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb106-45"><a href="#cb106-45" aria-hidden="true" tabindex="-1"></a>        all_param <span class="op">+=</span> param.numel()</span>
<span id="cb106-46"><a href="#cb106-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param.requires_grad:</span>
<span id="cb106-47"><a href="#cb106-47" aria-hidden="true" tabindex="-1"></a>            trainable_params <span class="op">+=</span> param.numel()</span>
<span id="cb106-48"><a href="#cb106-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb106-49"><a href="#cb106-49" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;trainable params: </span><span class="sc">{</span>trainable_params<span class="sc">}</span><span class="ss"> || all params: </span><span class="sc">{</span>all_param<span class="sc">}</span><span class="ss"> || trainable%: </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> trainable_params <span class="op">/</span> all_param<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb106-50"><a href="#cb106-50" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb106-51"><a href="#cb106-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-52"><a href="#cb106-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Create lora config.</span></span>
<span id="cb106-53"><a href="#cb106-53" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb106-54"><a href="#cb106-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-55"><a href="#cb106-55" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> LoraConfig(</span>
<span id="cb106-56"><a href="#cb106-56" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">8</span>, <span class="co">#attention dimension/rank</span></span>
<span id="cb106-57"><a href="#cb106-57" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">32</span>, <span class="co">#alpha scaling</span></span>
<span id="cb106-58"><a href="#cb106-58" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">&quot;qkv_proj&quot;</span>], <span class="co">#add the layers you to LoRA finetune</span></span>
<span id="cb106-59"><a href="#cb106-59" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb106-60"><a href="#cb106-60" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">&quot;none&quot;</span>,</span>
<span id="cb106-61"><a href="#cb106-61" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">&quot;CAUSAL_LM&quot;</span> <span class="co"># set this for CLM or Seq2Seq</span></span>
<span id="cb106-62"><a href="#cb106-62" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb106-63"><a href="#cb106-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-64"><a href="#cb106-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 3072x9216</span></span>
<span id="cb106-65"><a href="#cb106-65" aria-hidden="true" tabindex="-1"></a><span class="co"># 3072x8 8x9216</span></span>
<span id="cb106-66"><a href="#cb106-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-67"><a href="#cb106-67" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, config)</span>
<span id="cb106-68"><a href="#cb106-68" aria-hidden="true" tabindex="-1"></a>print_trainable_parameters(model)</span>
<span id="cb106-69"><a href="#cb106-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-70"><a href="#cb106-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-71"><a href="#cb106-71" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb107"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;6da503d2443a49308165f17cd99a15c9&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb108"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;5e32a307ca4644849cdb1e0bf41762b3&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:
- configuration_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb110"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;0a07be9678a944819523edea4c37450e&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:
- modeling_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named &#39;flash_attn&#39;.
WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation=&#39;eager&#39;`.
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb112"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;ed8e9e28b92943fd80d910f775bdec95&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb113"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;8fdf17c607df494486d0a7ec781bc50a&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb114"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;3c327f4e3a904634a739494db8e403f0&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb115"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;cf248685098149da8c7759d4ce865371&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb116"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;f617519da9e94764afe2d9dee9dd452e&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb117"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;dc16c496a71f40008f6be48b1210f5f5&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb118"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;24fc828d8f2c48209064323fba7c979f&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb119"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;e1a0ea7392eb4c3498ece72cb3337a93&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb120"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;ff03cbc374a440d988dba1992e4d138b&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb121"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;8f106423a25a4853b557c6db3b1f32de&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb122"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;9f9d2b6067e5438da93fc0681aa1cb16&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>trainable params: 3145728 || all params: 3824225280 || trainable%: 0.0822579155169436
</code></pre>
</div>
</div>
<div id="OLyy2zogosRH" class="cell code" data-execution_count="7"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535,&quot;referenced_widgets&quot;:[&quot;261339a05dfe49e6aea93a5dc67c8c3c&quot;,&quot;770b260482654c07807baa3e9f4773d7&quot;,&quot;e0aef3892c344e8d8d9c7227b1802d6a&quot;,&quot;a95a88595d664ee59d2ba78d69be0865&quot;,&quot;b15d0f28121c4460a41dd664be0cb097&quot;,&quot;90adcc3835964c52ab4a212eb3145957&quot;,&quot;6c953b9bcd0542cdbcf78da285d96824&quot;,&quot;6f4ea3e31ce1482fab979bbc35044021&quot;,&quot;08c7b7b39638484aa8a3074db873d59a&quot;,&quot;e4b0b7b9156645f38fcd103305d00228&quot;,&quot;212d9fa03ec541799d28481b5684109a&quot;]}"
id="OLyy2zogosRH" data-outputId="1d1395dc-ff5b-40e7-da3b-a8d871f3518b">
<div class="sourceCode" id="cb124"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co">#iport the data and do the necessary processing to prepare the dataset for training.</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>splits <span class="op">=</span> {<span class="st">&#39;train&#39;</span>: <span class="st">&#39;train.jsonl.gz&#39;</span>, <span class="st">&#39;validation&#39;</span>: <span class="st">&#39;validation.jsonl.gz&#39;</span>}</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_json(<span class="st">&quot;hf://datasets/nvidia/HelpSteer/&quot;</span> <span class="op">+</span> splits[<span class="st">&quot;train&quot;</span>], lines<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the DataFrame to a Dataset</span></span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>hf_dataset <span class="op">=</span> Dataset.from_pandas(df)</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">&quot;microsoft/Phi-3-mini-128k-instruct&quot;</span>)</span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-15"><a href="#cb124-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_function(examples):</span>
<span id="cb124-16"><a href="#cb124-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode the prompts and responses. We add special tokens to both input and target.</span></span>
<span id="cb124-17"><a href="#cb124-17" aria-hidden="true" tabindex="-1"></a>    model_inputs <span class="op">=</span> tokenizer(examples[<span class="st">&#39;prompt&#39;</span>], max_length<span class="op">=</span><span class="dv">512</span>, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">&quot;max_length&quot;</span>)</span>
<span id="cb124-18"><a href="#cb124-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-19"><a href="#cb124-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We need to specify that we&#39;re using the tokenizer for the targets to ensure it does not add special tokens</span></span>
<span id="cb124-20"><a href="#cb124-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tokenizer.as_target_tokenizer():</span>
<span id="cb124-21"><a href="#cb124-21" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> tokenizer(examples[<span class="st">&#39;response&#39;</span>], max_length<span class="op">=</span><span class="dv">128</span>, truncation<span class="op">=</span><span class="va">True</span>, padding<span class="op">=</span><span class="st">&quot;max_length&quot;</span>)</span>
<span id="cb124-22"><a href="#cb124-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-23"><a href="#cb124-23" aria-hidden="true" tabindex="-1"></a>    model_inputs[<span class="st">&#39;labels&#39;</span>] <span class="op">=</span> labels[<span class="st">&#39;input_ids&#39;</span>]</span>
<span id="cb124-24"><a href="#cb124-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model_inputs</span>
<span id="cb124-25"><a href="#cb124-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-26"><a href="#cb124-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the function to all splits in the dataset</span></span>
<span id="cb124-27"><a href="#cb124-27" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> hf_dataset.<span class="bu">map</span>(preprocess_function, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb124-28"><a href="#cb124-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and testing (or validation) sets</span></span>
<span id="cb124-29"><a href="#cb124-29" aria-hidden="true" tabindex="-1"></a>train_test_split <span class="op">=</span> tokenized_datasets.train_test_split(test_size<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb124-30"><a href="#cb124-30" aria-hidden="true" tabindex="-1"></a>dataset_dict <span class="op">=</span> {<span class="st">&quot;train&quot;</span>: train_test_split[<span class="st">&quot;train&quot;</span>], <span class="st">&quot;test&quot;</span>: train_test_split[<span class="st">&quot;test&quot;</span>]}</span>
<span id="cb124-31"><a href="#cb124-31" aria-hidden="true" tabindex="-1"></a>dataset_dict</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                                              prompt  \
0  What are the three most important things to co...   
1  What are the three most important things to co...   
2  What are the three most important things to co...   
3  What are the three most important things to co...   
4  Background:\n&lt;start of reference&gt;\nFamily doct...   

                                            response  helpfulness  \
0  To build an assistive device to help an elderl...            3   
1  There are many different types of assistive de...            4   
2  When deciding what technology to use to build ...            4   
3  You can create an assistant device to help an ...            3   
4  Hi there! I&#39;m Dr. Family, and I&#39;m here to tell...            3   

   correctness  coherence  complexity  verbosity  
0            4          4           2          2  
1            3          3           2          3  
2            4          4           2          2  
3            3          3           2          3  
4            3          3           2          1  
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb126"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;261339a05dfe49e6aea93a5dc67c8c3c&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output execute_result" data-execution_count="7">
<pre><code>{&#39;train&#39;: Dataset({
     features: [&#39;prompt&#39;, &#39;response&#39;, &#39;helpfulness&#39;, &#39;correctness&#39;, &#39;coherence&#39;, &#39;complexity&#39;, &#39;verbosity&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;],
     num_rows: 31797
 }),
 &#39;test&#39;: Dataset({
     features: [&#39;prompt&#39;, &#39;response&#39;, &#39;helpfulness&#39;, &#39;correctness&#39;, &#39;coherence&#39;, &#39;complexity&#39;, &#39;verbosity&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;],
     num_rows: 3534
 })}</code></pre>
</div>
</div>
<div id="_s-4bLnbou9U" class="cell code" data-execution_count="8"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="_s-4bLnbou9U" data-outputId="e98c61dd-c99f-401c-bd8e-61e03c52a806">
<div class="sourceCode" id="cb128"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># start training.</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> transformers.Trainer(</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset_dict[<span class="st">&#39;train&#39;</span>],</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>transformers.TrainingArguments(</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>        per_device_train_batch_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a>        gradient_accumulation_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a>        warmup_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a>        max_steps<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a>        fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a>        logging_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true" tabindex="-1"></a>        output_dir<span class="op">=</span><span class="st">&#39;outputs&#39;</span></span>
<span id="cb128-16"><a href="#cb128-16" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb128-17"><a href="#cb128-17" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>transformers.DataCollatorForLanguageModeling(tokenizer, mlm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb128-18"><a href="#cb128-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-19"><a href="#cb128-19" aria-hidden="true" tabindex="-1"></a>model.config.use_cache <span class="op">=</span> <span class="va">False</span>  <span class="co"># silence the warnings. Please re-enable for inference!</span></span>
<span id="cb128-20"><a href="#cb128-20" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb128-21"><a href="#cb128-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-22"><a href="#cb128-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dir to save the model and save the model and tokenizer in that directory.</span></span>
<span id="cb128-23"><a href="#cb128-23" aria-hidden="true" tabindex="-1"></a>os.mkdir(<span class="st">&quot;/content/model_repo&quot;</span>)</span>
<span id="cb128-24"><a href="#cb128-24" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="st">&#39;/content/model_repo&#39;</span>)</span>
<span id="cb128-25"><a href="#cb128-25" aria-hidden="true" tabindex="-1"></a>tokenizer.save_pretrained(<span class="st">&#39;/content/model_repo&#39;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>max_steps is given, it will override any value given in num_train_epochs
WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.
</code></pre>
</div>
<div class="output display_data">

    <div>
      
      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [200/200 01:51, Epoch 0/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>3.417600</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.715700</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2.173200</td>
    </tr>
    <tr>
      <td>4</td>
      <td>2.681000</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2.328500</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1.849100</td>
    </tr>
    <tr>
      <td>7</td>
      <td>2.691700</td>
    </tr>
    <tr>
      <td>8</td>
      <td>2.029000</td>
    </tr>
    <tr>
      <td>9</td>
      <td>1.865900</td>
    </tr>
    <tr>
      <td>10</td>
      <td>1.699100</td>
    </tr>
    <tr>
      <td>11</td>
      <td>3.427900</td>
    </tr>
    <tr>
      <td>12</td>
      <td>2.555400</td>
    </tr>
    <tr>
      <td>13</td>
      <td>2.839200</td>
    </tr>
    <tr>
      <td>14</td>
      <td>2.407700</td>
    </tr>
    <tr>
      <td>15</td>
      <td>4.806600</td>
    </tr>
    <tr>
      <td>16</td>
      <td>2.179700</td>
    </tr>
    <tr>
      <td>17</td>
      <td>2.223200</td>
    </tr>
    <tr>
      <td>18</td>
      <td>3.673100</td>
    </tr>
    <tr>
      <td>19</td>
      <td>1.204200</td>
    </tr>
    <tr>
      <td>20</td>
      <td>1.774800</td>
    </tr>
    <tr>
      <td>21</td>
      <td>1.727800</td>
    </tr>
    <tr>
      <td>22</td>
      <td>1.731600</td>
    </tr>
    <tr>
      <td>23</td>
      <td>2.080800</td>
    </tr>
    <tr>
      <td>24</td>
      <td>1.342900</td>
    </tr>
    <tr>
      <td>25</td>
      <td>1.871000</td>
    </tr>
    <tr>
      <td>26</td>
      <td>1.384600</td>
    </tr>
    <tr>
      <td>27</td>
      <td>2.047500</td>
    </tr>
    <tr>
      <td>28</td>
      <td>1.000500</td>
    </tr>
    <tr>
      <td>29</td>
      <td>1.781800</td>
    </tr>
    <tr>
      <td>30</td>
      <td>1.995000</td>
    </tr>
    <tr>
      <td>31</td>
      <td>2.138400</td>
    </tr>
    <tr>
      <td>32</td>
      <td>1.960100</td>
    </tr>
    <tr>
      <td>33</td>
      <td>2.299700</td>
    </tr>
    <tr>
      <td>34</td>
      <td>2.356600</td>
    </tr>
    <tr>
      <td>35</td>
      <td>2.288000</td>
    </tr>
    <tr>
      <td>36</td>
      <td>2.058100</td>
    </tr>
    <tr>
      <td>37</td>
      <td>2.204700</td>
    </tr>
    <tr>
      <td>38</td>
      <td>2.101600</td>
    </tr>
    <tr>
      <td>39</td>
      <td>2.161300</td>
    </tr>
    <tr>
      <td>40</td>
      <td>1.838800</td>
    </tr>
    <tr>
      <td>41</td>
      <td>2.197800</td>
    </tr>
    <tr>
      <td>42</td>
      <td>2.304500</td>
    </tr>
    <tr>
      <td>43</td>
      <td>1.984400</td>
    </tr>
    <tr>
      <td>44</td>
      <td>1.706200</td>
    </tr>
    <tr>
      <td>45</td>
      <td>2.623000</td>
    </tr>
    <tr>
      <td>46</td>
      <td>1.996000</td>
    </tr>
    <tr>
      <td>47</td>
      <td>2.220200</td>
    </tr>
    <tr>
      <td>48</td>
      <td>2.303500</td>
    </tr>
    <tr>
      <td>49</td>
      <td>2.088000</td>
    </tr>
    <tr>
      <td>50</td>
      <td>2.009000</td>
    </tr>
    <tr>
      <td>51</td>
      <td>2.279000</td>
    </tr>
    <tr>
      <td>52</td>
      <td>1.808400</td>
    </tr>
    <tr>
      <td>53</td>
      <td>2.414800</td>
    </tr>
    <tr>
      <td>54</td>
      <td>2.750700</td>
    </tr>
    <tr>
      <td>55</td>
      <td>2.279800</td>
    </tr>
    <tr>
      <td>56</td>
      <td>1.460700</td>
    </tr>
    <tr>
      <td>57</td>
      <td>1.968800</td>
    </tr>
    <tr>
      <td>58</td>
      <td>1.915300</td>
    </tr>
    <tr>
      <td>59</td>
      <td>2.371600</td>
    </tr>
    <tr>
      <td>60</td>
      <td>1.394700</td>
    </tr>
    <tr>
      <td>61</td>
      <td>2.196500</td>
    </tr>
    <tr>
      <td>62</td>
      <td>1.990900</td>
    </tr>
    <tr>
      <td>63</td>
      <td>1.866200</td>
    </tr>
    <tr>
      <td>64</td>
      <td>1.425100</td>
    </tr>
    <tr>
      <td>65</td>
      <td>0.915800</td>
    </tr>
    <tr>
      <td>66</td>
      <td>1.526600</td>
    </tr>
    <tr>
      <td>67</td>
      <td>2.408200</td>
    </tr>
    <tr>
      <td>68</td>
      <td>1.908600</td>
    </tr>
    <tr>
      <td>69</td>
      <td>3.067900</td>
    </tr>
    <tr>
      <td>70</td>
      <td>2.224900</td>
    </tr>
    <tr>
      <td>71</td>
      <td>1.799700</td>
    </tr>
    <tr>
      <td>72</td>
      <td>2.048700</td>
    </tr>
    <tr>
      <td>73</td>
      <td>1.800800</td>
    </tr>
    <tr>
      <td>74</td>
      <td>2.081400</td>
    </tr>
    <tr>
      <td>75</td>
      <td>2.618100</td>
    </tr>
    <tr>
      <td>76</td>
      <td>2.212400</td>
    </tr>
    <tr>
      <td>77</td>
      <td>0.998600</td>
    </tr>
    <tr>
      <td>78</td>
      <td>2.753300</td>
    </tr>
    <tr>
      <td>79</td>
      <td>2.702100</td>
    </tr>
    <tr>
      <td>80</td>
      <td>1.756200</td>
    </tr>
    <tr>
      <td>81</td>
      <td>1.843400</td>
    </tr>
    <tr>
      <td>82</td>
      <td>1.661900</td>
    </tr>
    <tr>
      <td>83</td>
      <td>2.415500</td>
    </tr>
    <tr>
      <td>84</td>
      <td>2.148700</td>
    </tr>
    <tr>
      <td>85</td>
      <td>2.177100</td>
    </tr>
    <tr>
      <td>86</td>
      <td>1.503600</td>
    </tr>
    <tr>
      <td>87</td>
      <td>2.084300</td>
    </tr>
    <tr>
      <td>88</td>
      <td>2.044400</td>
    </tr>
    <tr>
      <td>89</td>
      <td>1.715600</td>
    </tr>
    <tr>
      <td>90</td>
      <td>1.786500</td>
    </tr>
    <tr>
      <td>91</td>
      <td>1.179900</td>
    </tr>
    <tr>
      <td>92</td>
      <td>2.036900</td>
    </tr>
    <tr>
      <td>93</td>
      <td>1.440200</td>
    </tr>
    <tr>
      <td>94</td>
      <td>3.419400</td>
    </tr>
    <tr>
      <td>95</td>
      <td>2.257800</td>
    </tr>
    <tr>
      <td>96</td>
      <td>2.614000</td>
    </tr>
    <tr>
      <td>97</td>
      <td>1.846500</td>
    </tr>
    <tr>
      <td>98</td>
      <td>2.461900</td>
    </tr>
    <tr>
      <td>99</td>
      <td>2.506400</td>
    </tr>
    <tr>
      <td>100</td>
      <td>2.107900</td>
    </tr>
    <tr>
      <td>101</td>
      <td>1.941600</td>
    </tr>
    <tr>
      <td>102</td>
      <td>1.077800</td>
    </tr>
    <tr>
      <td>103</td>
      <td>1.825600</td>
    </tr>
    <tr>
      <td>104</td>
      <td>2.533300</td>
    </tr>
    <tr>
      <td>105</td>
      <td>1.568900</td>
    </tr>
    <tr>
      <td>106</td>
      <td>0.897300</td>
    </tr>
    <tr>
      <td>107</td>
      <td>0.948500</td>
    </tr>
    <tr>
      <td>108</td>
      <td>1.419500</td>
    </tr>
    <tr>
      <td>109</td>
      <td>1.681400</td>
    </tr>
    <tr>
      <td>110</td>
      <td>2.303600</td>
    </tr>
    <tr>
      <td>111</td>
      <td>0.925600</td>
    </tr>
    <tr>
      <td>112</td>
      <td>1.046800</td>
    </tr>
    <tr>
      <td>113</td>
      <td>2.206400</td>
    </tr>
    <tr>
      <td>114</td>
      <td>2.552200</td>
    </tr>
    <tr>
      <td>115</td>
      <td>2.266100</td>
    </tr>
    <tr>
      <td>116</td>
      <td>2.072500</td>
    </tr>
    <tr>
      <td>117</td>
      <td>1.718300</td>
    </tr>
    <tr>
      <td>118</td>
      <td>1.819900</td>
    </tr>
    <tr>
      <td>119</td>
      <td>1.744300</td>
    </tr>
    <tr>
      <td>120</td>
      <td>2.694100</td>
    </tr>
    <tr>
      <td>121</td>
      <td>2.649400</td>
    </tr>
    <tr>
      <td>122</td>
      <td>1.051200</td>
    </tr>
    <tr>
      <td>123</td>
      <td>1.160300</td>
    </tr>
    <tr>
      <td>124</td>
      <td>1.003300</td>
    </tr>
    <tr>
      <td>125</td>
      <td>2.636700</td>
    </tr>
    <tr>
      <td>126</td>
      <td>1.874300</td>
    </tr>
    <tr>
      <td>127</td>
      <td>1.743200</td>
    </tr>
    <tr>
      <td>128</td>
      <td>2.222700</td>
    </tr>
    <tr>
      <td>129</td>
      <td>2.344300</td>
    </tr>
    <tr>
      <td>130</td>
      <td>2.059700</td>
    </tr>
    <tr>
      <td>131</td>
      <td>2.402000</td>
    </tr>
    <tr>
      <td>132</td>
      <td>1.944200</td>
    </tr>
    <tr>
      <td>133</td>
      <td>0.879000</td>
    </tr>
    <tr>
      <td>134</td>
      <td>1.673100</td>
    </tr>
    <tr>
      <td>135</td>
      <td>1.543000</td>
    </tr>
    <tr>
      <td>136</td>
      <td>1.531900</td>
    </tr>
    <tr>
      <td>137</td>
      <td>1.279700</td>
    </tr>
    <tr>
      <td>138</td>
      <td>2.006900</td>
    </tr>
    <tr>
      <td>139</td>
      <td>1.769900</td>
    </tr>
    <tr>
      <td>140</td>
      <td>2.429300</td>
    </tr>
    <tr>
      <td>141</td>
      <td>1.267700</td>
    </tr>
    <tr>
      <td>142</td>
      <td>2.558700</td>
    </tr>
    <tr>
      <td>143</td>
      <td>1.872600</td>
    </tr>
    <tr>
      <td>144</td>
      <td>2.728600</td>
    </tr>
    <tr>
      <td>145</td>
      <td>1.620800</td>
    </tr>
    <tr>
      <td>146</td>
      <td>2.066500</td>
    </tr>
    <tr>
      <td>147</td>
      <td>1.946900</td>
    </tr>
    <tr>
      <td>148</td>
      <td>2.461600</td>
    </tr>
    <tr>
      <td>149</td>
      <td>2.196600</td>
    </tr>
    <tr>
      <td>150</td>
      <td>1.909300</td>
    </tr>
    <tr>
      <td>151</td>
      <td>1.046400</td>
    </tr>
    <tr>
      <td>152</td>
      <td>1.918900</td>
    </tr>
    <tr>
      <td>153</td>
      <td>1.810000</td>
    </tr>
    <tr>
      <td>154</td>
      <td>1.628400</td>
    </tr>
    <tr>
      <td>155</td>
      <td>1.374300</td>
    </tr>
    <tr>
      <td>156</td>
      <td>2.122800</td>
    </tr>
    <tr>
      <td>157</td>
      <td>1.414800</td>
    </tr>
    <tr>
      <td>158</td>
      <td>1.566800</td>
    </tr>
    <tr>
      <td>159</td>
      <td>1.509700</td>
    </tr>
    <tr>
      <td>160</td>
      <td>2.211800</td>
    </tr>
    <tr>
      <td>161</td>
      <td>1.204900</td>
    </tr>
    <tr>
      <td>162</td>
      <td>3.067900</td>
    </tr>
    <tr>
      <td>163</td>
      <td>2.338600</td>
    </tr>
    <tr>
      <td>164</td>
      <td>1.819000</td>
    </tr>
    <tr>
      <td>165</td>
      <td>2.394400</td>
    </tr>
    <tr>
      <td>166</td>
      <td>1.064200</td>
    </tr>
    <tr>
      <td>167</td>
      <td>2.124800</td>
    </tr>
    <tr>
      <td>168</td>
      <td>2.013000</td>
    </tr>
    <tr>
      <td>169</td>
      <td>2.318500</td>
    </tr>
    <tr>
      <td>170</td>
      <td>1.732300</td>
    </tr>
    <tr>
      <td>171</td>
      <td>2.108500</td>
    </tr>
    <tr>
      <td>172</td>
      <td>2.110600</td>
    </tr>
    <tr>
      <td>173</td>
      <td>1.744000</td>
    </tr>
    <tr>
      <td>174</td>
      <td>1.000500</td>
    </tr>
    <tr>
      <td>175</td>
      <td>1.962900</td>
    </tr>
    <tr>
      <td>176</td>
      <td>1.218800</td>
    </tr>
    <tr>
      <td>177</td>
      <td>1.789700</td>
    </tr>
    <tr>
      <td>178</td>
      <td>1.568300</td>
    </tr>
    <tr>
      <td>179</td>
      <td>1.572400</td>
    </tr>
    <tr>
      <td>180</td>
      <td>2.202100</td>
    </tr>
    <tr>
      <td>181</td>
      <td>1.886400</td>
    </tr>
    <tr>
      <td>182</td>
      <td>2.520700</td>
    </tr>
    <tr>
      <td>183</td>
      <td>0.960000</td>
    </tr>
    <tr>
      <td>184</td>
      <td>2.606900</td>
    </tr>
    <tr>
      <td>185</td>
      <td>1.863400</td>
    </tr>
    <tr>
      <td>186</td>
      <td>1.704700</td>
    </tr>
    <tr>
      <td>187</td>
      <td>2.277200</td>
    </tr>
    <tr>
      <td>188</td>
      <td>0.902600</td>
    </tr>
    <tr>
      <td>189</td>
      <td>1.910100</td>
    </tr>
    <tr>
      <td>190</td>
      <td>1.853400</td>
    </tr>
    <tr>
      <td>191</td>
      <td>3.499200</td>
    </tr>
    <tr>
      <td>192</td>
      <td>2.104500</td>
    </tr>
    <tr>
      <td>193</td>
      <td>1.968800</td>
    </tr>
    <tr>
      <td>194</td>
      <td>1.801200</td>
    </tr>
    <tr>
      <td>195</td>
      <td>2.436500</td>
    </tr>
    <tr>
      <td>196</td>
      <td>0.808200</td>
    </tr>
    <tr>
      <td>197</td>
      <td>1.877900</td>
    </tr>
    <tr>
      <td>198</td>
      <td>1.330000</td>
    </tr>
    <tr>
      <td>199</td>
      <td>2.421300</td>
    </tr>
    <tr>
      <td>200</td>
      <td>1.286500</td>
    </tr>
  </tbody>
</table><p>
</div>
<div class="output execute_result" data-execution_count="8">
<pre><code>(&#39;/content/model_repo/tokenizer_config.json&#39;,
 &#39;/content/model_repo/special_tokens_map.json&#39;,
 &#39;/content/model_repo/tokenizer.model&#39;,
 &#39;/content/model_repo/added_tokens.json&#39;,
 &#39;/content/model_repo/tokenizer.json&#39;)</code></pre>
</div>
</div>
<div id="vMwyhZSTqk-S" class="cell code" data-execution_count="9"
id="vMwyhZSTqk-S">
<div class="sourceCode" id="cb131"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># delete the model from runtime.</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> model</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> tokenizer</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> trainer</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code></pre></div>
</div>
<div id="lFN9hxN1q4ng" class="cell code" data-execution_count="10"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:49,&quot;referenced_widgets&quot;:[&quot;d03a015f45d043d3a100054670108e53&quot;,&quot;cdfe3dfafc8d4f14b1658633efc28ff4&quot;,&quot;c22e595f6a384ccabb7110469b2ab500&quot;,&quot;0ecb09e4d25344f3b0cf4f288a46d181&quot;,&quot;0b3c4be94a0e487b9433f85868f053c4&quot;,&quot;5c77c45968004b10a40a8c99170f11f9&quot;,&quot;9e7beed9d4424c1aa350a0e5c062427b&quot;,&quot;f8620c83c1f941de9ef213a33e6b667c&quot;,&quot;193a3d1b27854c75b298ec1d0686fb99&quot;,&quot;b2b9474cc12843cc8fada165e2328427&quot;,&quot;dc8e607e8d164036b684f9cc2fc81514&quot;]}"
id="lFN9hxN1q4ng" data-outputId="3cb30826-b91d-4485-c3bb-f1fc3aebeb9e">
<div class="sourceCode" id="cb132"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Load the model from local</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> PeftModel, PeftConfig</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the local path where your model and tokenizer are stored</span></span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>local_model_path <span class="op">=</span> <span class="st">&#39;/content/model_repo/&#39;</span></span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create quantization config</span></span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>quantization_config <span class="op">=</span> BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the PEFT configuration from the local path</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> PeftConfig.from_pretrained(local_model_path)</span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the base model and tokenizer with quantization config</span></span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(local_model_path, return_dict<span class="op">=</span><span class="va">True</span>, quantization_config<span class="op">=</span>quantization_config, device_map<span class="op">=</span><span class="st">&#39;auto&#39;</span>)</span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(local_model_path)</span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the LoRA model adjustments from the same local path (assuming PEFT adjustments are saved in the same directory)</span></span>
<span id="cb132-20"><a href="#cb132-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: This assumes that the PeftModel class can handle loading from a local path; this may require additional implementation depending on the PEFT library specifics</span></span>
<span id="cb132-21"><a href="#cb132-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PeftModel.from_pretrained(model, local_model_path)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb133"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;d03a015f45d043d3a100054670108e53&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div id="PMJ9LninrBTh" class="cell code" data-execution_count="11"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="PMJ9LninrBTh" data-outputId="a91fd8a6-d99b-4af4-b9c1-82f4898a96d7">
<div class="sourceCode" id="cb134"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check the output for an example.</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(<span class="st">&quot;&lt;|prompt|&gt;</span><span class="ch">\n</span><span class="st">What are the three most important things to consider when deciding what technology to use to build an assist device to help an elderly person with basic needs?&lt;|end|&gt;</span><span class="ch">\n</span><span class="st">&lt;|response|&gt;&quot;</span>, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.cuda.amp.autocast():</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>  output_tokens <span class="op">=</span> model.generate(<span class="op">**</span>batch, max_new_tokens<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n\n</span><span class="st"> response </span><span class="ch">\n</span><span class="st"> &#39;</span>, tokenizer.decode(output_tokens[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>))</span></code></pre></div>
<div class="output stream stderr">
<pre><code>You are not running the flash-attention implementation, expect numerical differences.
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>

 response 
  &lt;|prompt|&gt;
What are the three most important things to consider when deciding what technology to use to build an assist device to help an elderly person with basic needs? &lt;|response|&gt;
When deciding on technology to build an assist device for an elderly person with basic needs, there are three critical factors to consider:

1. User-friendlinemen: The technology should be easy to use and understand for the elder
</code></pre>
</div>
</div>
<div id="3283681a" class="cell markdown" id="3283681a">
<h2
id="8-implement-a-function-to-evaluate-the-models-performance-using-metrics-such-as-accuracy-and-f1-score-1-mark">8.
Implement a function to evaluate the model's performance using metrics
such as accuracy and F1 score. [1 Mark]</h2>
</div>
<div id="70e3a17e" class="cell markdown" id="70e3a17e">
<h2
id="9-analyze-the-results-to-discuss-the-impact-of-peft-on-model-performance-and-efficiency-1-mark">9.
Analyze the results to discuss the impact of PEFT on model performance
and efficiency. [1 Mark]</h2>
</div>
<div id="10d5cb6a" class="cell code" id="10d5cb6a">
<div class="sourceCode" id="cb137"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
<script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script></body>
</html>
