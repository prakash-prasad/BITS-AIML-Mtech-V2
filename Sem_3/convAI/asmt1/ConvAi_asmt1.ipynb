{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3d14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git -q\n",
    "!pip install bitsandbytes datasets accelerate loralib -q\n",
    "# !pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu11.8/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac15894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8618f51372434a419ac841291d00b94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739ea420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from accelerate import Accelerator\n",
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, RobertaTokenizer, GPT2Tokenizer, \\\n",
    "    GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "# torch.cuda.set_device(0)\n",
    "# # torch.cuda.set_device(1)\n",
    "# # torch.cuda.set_device(2)\n",
    "# # torch.cuda.set_device(3)\n",
    "# torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a6255b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e0129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69089fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "ds = load_dataset(\"nvidia/HelpSteer2\")\n",
    "train = ds['train'] # len(train) = 20324 (95%)\n",
    "val = ds['validation']     # len(val) = 1038 (5%)\n",
    "\n",
    "train_pd = pd.DataFrame(train)\n",
    "val_pd = pd.DataFrame(val)\n",
    "train_pd.head()\n",
    "val_pd.head()\n",
    "\n",
    "train_df = train_pd.copy()\n",
    "val_df = val_pd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa142bb8",
   "metadata": {},
   "source": [
    "## 1. no preprocessing needed for llm , lm preprocessing for lstm: [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c678645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = train_pd[:500]\n",
    "val_pd = val_pd[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5171e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length of prompts and response:  548 1045\n",
      "Vocabulary size from train prompt + response  7095\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompts and answers\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_pd['prompt'].tolist() + train_pd['response'].tolist()),\n",
    "                                  specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Convert texts to sequences\n",
    "def text_pipeline(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "train_prompt_sequences = [text_pipeline(text) for text in train_pd['prompt']]\n",
    "train_answer_sequences = [text_pipeline(text) for text in train_pd['response']]\n",
    "val_prompt_sequences = [text_pipeline(text) for text in val_pd['prompt']]\n",
    "val_answer_sequences = [text_pipeline(text) for text in val_pd['response']]\n",
    "\n",
    "max_len_prompt = max(len(seq) for seq in train_prompt_sequences)\n",
    "max_len_answer = max(len(seq) for seq in train_answer_sequences)\n",
    "print('max token length of prompts and response: ', max_len_prompt, max_len_answer)\n",
    "# Vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "print('Vocabulary size from train prompt + response ', vocab_size)\n",
    "\n",
    "# Pad the sequences\n",
    "def pad_sequences(sequences, max_len):\n",
    "    return pad_sequence([torch.tensor(seq[:max_len]) for seq in sequences], batch_first=True,\n",
    "                        padding_value=vocab[\"<unk>\"])\n",
    "\n",
    "train_prompt_sequences = pad_sequences(train_prompt_sequences, max_len_prompt)\n",
    "train_answer_sequences = pad_sequences(train_answer_sequences, max_len_answer)\n",
    "val_prompt_sequences = pad_sequences(val_prompt_sequences, max_len_prompt)\n",
    "val_answer_sequences = pad_sequences(val_answer_sequences, max_len_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "777b6a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300, 548]),\n",
       " torch.Size([300, 1045]),\n",
       " torch.Size([50, 387]),\n",
       " torch.Size([50, 808]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompt_sequences.shape, train_answer_sequences.shape, val_prompt_sequences.shape, val_answer_sequences.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e8b088",
   "metadata": {},
   "source": [
    "Not sure why seq length isnt same for train and val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f5d9c",
   "metadata": {},
   "source": [
    "## 2.Explain and implement how LLM (2B to 8B parameters) over simple LM can help improve the system accuracy. [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12573dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 2 Seq LM using lstm on Pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, hidden):\n",
    "        encoder_embedded = self.embedding(encoder_input)\n",
    "        encoder_output, (hidden, cell) = self.encoder_lstm(encoder_embedded, hidden)\n",
    "        \n",
    "        decoder_embedded = self.embedding(decoder_input)\n",
    "        decoder_output, (hidden, cell) = self.decoder_lstm(decoder_embedded, (hidden, cell))\n",
    "        \n",
    "        output = self.fc(decoder_output)\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "embedding_dim = 128\n",
    "hidden_units = 256\n",
    "\n",
    "model = Seq2Seq(vocab_size, embedding_dim, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9b8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<unk>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device='cpu' # using cpu since gpu gives memory error\n",
    "# Move model to GPU\n",
    "# device_ids = [0, 1, 2, 3]  # Replace with desired GPU IDs\n",
    "# model = nn.DataParallel(model, device_ids=device_ids)\n",
    "# model.to(f'cuda:{device_ids[0]}')  # Move the model to the first GPU\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # input_tensor = input_tensor.to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "# Prepare the target data for the decoder (shifted one time step) !!!\n",
    "def prepare_target_sequences(sequences):\n",
    "    inputs = sequences[:, :-1]\n",
    "    targets = sequences[:, 1:]\n",
    "    return inputs, targets\n",
    "   \n",
    "train_answer_sequences_input, train_answer_sequences_output = prepare_target_sequences(train_answer_sequences)\n",
    "val_answer_sequences_input, val_answer_sequences_output = prepare_target_sequences(val_answer_sequences)\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "train_prompt_sequences = train_prompt_sequences.to(device)\n",
    "train_answer_sequences_input = train_answer_sequences_input.to(device)\n",
    "train_answer_sequences_output = train_answer_sequences_output.to(device)\n",
    "val_prompt_sequences = val_prompt_sequences.to(device)\n",
    "val_answer_sequences_input = val_answer_sequences_input.to(device)\n",
    "val_answer_sequences_output = val_answer_sequences_output.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6819aee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 8.87558364868164\n",
      "Epoch [2/50], Loss: 8.849912643432617\n",
      "Epoch [3/50], Loss: 8.822985649108887\n",
      "Epoch [4/50], Loss: 8.792208671569824\n",
      "Epoch [5/50], Loss: 8.754260063171387\n",
      "Epoch [6/50], Loss: 8.703964233398438\n",
      "Epoch [7/50], Loss: 8.631867408752441\n",
      "Epoch [8/50], Loss: 8.517773628234863\n",
      "Epoch [9/50], Loss: 8.311432838439941\n",
      "Epoch [10/50], Loss: 7.934554100036621\n",
      "Epoch [11/50], Loss: 7.5272321701049805\n",
      "Epoch [12/50], Loss: 7.2270894050598145\n",
      "Epoch [13/50], Loss: 7.004649639129639\n",
      "Epoch [14/50], Loss: 6.82792854309082\n",
      "Epoch [15/50], Loss: 6.6866374015808105\n",
      "Epoch [16/50], Loss: 6.579782009124756\n",
      "Epoch [17/50], Loss: 6.508011341094971\n",
      "Epoch [18/50], Loss: 6.469689846038818\n",
      "Epoch [19/50], Loss: 6.459895610809326\n",
      "Epoch [20/50], Loss: 6.468969345092773\n",
      "Epoch [21/50], Loss: 6.484289169311523\n",
      "Epoch [22/50], Loss: 6.495165824890137\n",
      "Epoch [23/50], Loss: 6.496269226074219\n",
      "Epoch [24/50], Loss: 6.487391948699951\n",
      "Epoch [25/50], Loss: 6.471380233764648\n",
      "Epoch [26/50], Loss: 6.452081203460693\n",
      "Epoch [27/50], Loss: 6.432642936706543\n",
      "Epoch [28/50], Loss: 6.4148335456848145\n",
      "Epoch [29/50], Loss: 6.399575710296631\n",
      "Epoch [30/50], Loss: 6.3866286277771\n",
      "Epoch [31/50], Loss: 6.375542163848877\n",
      "Epoch [32/50], Loss: 6.36587381362915\n",
      "Epoch [33/50], Loss: 6.35689115524292\n",
      "Epoch [34/50], Loss: 6.3482255935668945\n",
      "Epoch [35/50], Loss: 6.339582920074463\n",
      "Epoch [36/50], Loss: 6.3308844566345215\n",
      "Epoch [37/50], Loss: 6.321742534637451\n",
      "Epoch [38/50], Loss: 6.312087059020996\n",
      "Epoch [39/50], Loss: 6.30206298828125\n",
      "Epoch [40/50], Loss: 6.291914463043213\n",
      "Epoch [41/50], Loss: 6.281908988952637\n",
      "Epoch [42/50], Loss: 6.272091865539551\n",
      "Epoch [43/50], Loss: 6.262415885925293\n",
      "Epoch [44/50], Loss: 6.2526750564575195\n",
      "Epoch [45/50], Loss: 6.242735862731934\n",
      "Epoch [46/50], Loss: 6.2325520515441895\n",
      "Epoch [47/50], Loss: 6.222198486328125\n",
      "Epoch [48/50], Loss: 6.211533546447754\n",
      "Epoch [49/50], Loss: 6.200573921203613\n",
      "Epoch [50/50], Loss: 6.1894450187683105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "466.6952950954437"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "init = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output, _ = model(train_prompt_sequences, train_answer_sequences_input, None)\n",
    "#     loss = criterion(output.view(-1, vocab_size), train_answer_sequences_output.view(-1)) # ask\n",
    "    loss = criterion(output.reshape(-1, vocab_size), train_answer_sequences_output.reshape(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "time.time() - init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c5bad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'lstm_lm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d1c1613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (embedding): Embedding(7095, 128)\n",
       "  (encoder_lstm): LSTM(128, 256, batch_first=True)\n",
       "  (decoder_lstm): LSTM(128, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=7095, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('lstm_lm_model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "630d4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.111302852630615\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_output, _ = model(val_prompt_sequences, val_answer_sequences_input, None)\n",
    "#     val_loss = criterion(val_output.view(-1, vocab_size), val_answer_sequences_output.view(-1))\n",
    "    val_loss = criterion(val_output.reshape(-1, vocab_size), val_answer_sequences_output.reshape(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0f5c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_lm_response(prompt, model, vocab, max_len_answer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prompt_sequence = pad_sequence([torch.tensor(text_pipeline(prompt))[:max_len_prompt]], batch_first=True, padding_value=vocab[\"<unk>\"]).to(device)\n",
    "        decoder_input = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "        \n",
    "        hidden = None\n",
    "        decoded_sentence = []\n",
    "\n",
    "        for _ in range(max_len_answer):\n",
    "            output, hidden = model(prompt_sequence, decoder_input, hidden)\n",
    "            sampled_token_index = output.argmax(2)[:, -1].item()\n",
    "            sampled_word = vocab.lookup_token(sampled_token_index)\n",
    "            decoded_sentence.append(sampled_word)\n",
    "            if sampled_word == '<end>':\n",
    "                break\n",
    "            decoder_input = torch.cat([decoder_input, torch.tensor([[sampled_token_index]], dtype=torch.long).to(device)], dim=1)\n",
    "            \n",
    "        return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "787425df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please give us creative product ideas related to air fresheners.\n",
      ", and , and , and , and , and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the\n"
     ]
    }
   ],
   "source": [
    "# Generate an answer - simple LM\n",
    "prompt_ = val_pd['prompt'][9]\n",
    "print(prompt_)\n",
    "print(generate_simple_lm_response(prompt_, model, vocab, max_len_answer))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5d487ed",
   "metadata": {},
   "source": [
    "using lstm with limited training gives gibberish answers\n",
    "using LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375bcfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e4e1fe53154b9e880f43861f0accf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81910856e7964587a716fc2b4b6d2708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9987e82ce73a4811a17b784163932a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc20eccec8e45ef8c1267db4a5bcb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e210d097f44b9f82fab90dc5c1f37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbee5f234f8407fa6cb47b3f19499db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0742746fe6274962b5f0244fa8ee2dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5c7ee39236454a9b735f258669e20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac010741448483fb5991295b9533daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53672949fecc455899fd961f3b711f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155ea8721fb64262982addf313ca7098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7729e598544f40bdbe83e93e7d2c3666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name, add_bos_token=False, add_eos_token=True)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name)\n",
    "device = 'cpu'\n",
    "print(device)\n",
    "llm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff0a1eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "meta lamma tokeniser output:  ['Please', 'Ä give', 'Ä us', 'Ä creative', 'Ä product', 'Ä ideas', 'Ä related', 'Ä to', 'Ä air', 'Ä fresh', 'eners', '.']\n",
      "generated response:  Please give us creative product ideas related to air fresheners. I am looking for unique, new and innovative ideas for air fresheners. These ideas should be innovative and creative.\n"
     ]
    }
   ],
   "source": [
    "def generate_llm_responses(text_):\n",
    "    inputs = llm_tokenizer.encode_plus(text_, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                                 num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt_ = val_pd['prompt'][9]\n",
    "print('prompt: ', prompt_)\n",
    "print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28a72e0e",
   "metadata": {},
   "source": [
    "clearly llm works better - implementation above, explaination - self attention etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d873952a",
   "metadata": {},
   "source": [
    "## 3.\tExplain and implement how specific tokenizer and model can work well with the mentioned dataset. [1 Mark]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ae8750f",
   "metadata": {},
   "source": [
    "3 tokenisers - one comes directly with meta-lamma-3-8b, we use byte pair encoder tokeniser ((RobertaTokenizer) and one Character-Level Tokenizer and compare output of llm on all 3.\n",
    "\n",
    "first part is already done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efdabdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe169d45364eca95d5b4c378592ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278c46a5a304331be40e70cffdaca20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b73725720a6434495c6da363fc64f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62600a008844055b147594bde7c75ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58759c35d8874ad2a46c965b84fb1677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenized Prompts:  ['Please', 'Ä give', 'Ä us', 'Ä creative', 'Ä product', 'Ä ideas', 'Ä related', 'Ä to', 'Ä air', 'Ä fres', 'hen', 'ers', '.']\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "bpe_tokenized_prompt_ = bpe_tokenizer.tokenize(prompt_)\n",
    "print(\"BPE Tokenized Prompts: \", bpe_tokenized_prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "613e8cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-Level Tokenized Prompts:  ['explain', 'master', 'slave', 'replication', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "char_tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "char_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "char_tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "vocab_size_=vocab_size\n",
    "# Train the tokenizer on the dataset\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=vocab_size_, special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "char_tokenizer.train_from_iterator(train_df['prompt'], trainer)\n",
    "\n",
    "# Tokenize using the character-level tokenizer\n",
    "char_tokenized_prompt_ = char_tokenizer.encode(prompt).tokens\n",
    "print(\"Character-Level Tokenized Prompts: \", char_tokenized_prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "635a856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "bpe encoded response:  Please give us creative product ideas related to air fresheners. next's for is forï¿½ forï¿½ us. next's for is forï¿½ for's us. next's for is forï¿½ for with us. next's for is forï¿½ for The us. next's for is forï¿½ for was us. next's for is forï¿½ for \" us. next's for is forï¿½ for at us. next's for is forï¿½ for it us. next's for is forï¿½ forw us. next's for is forï¿½ for technology us. next's for is forï¿½ forE us. next's for is\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using BPE tokenizer\n",
    "inputs = bpe_tokenizer.encode_plus(prompt_, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                             num_return_sequences=1, pad_token_id=bpe_tokenizer.eos_token_id)\n",
    "response = bpe_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print('prompt: ', prompt_)\n",
    "# print('\\n')\n",
    "print('bpe encoded response: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate predictions using BPE tokenizer\n",
    "# inputs = bpe_tokenizer.encode_plus(prompt_, return_tensors='pt', padding=True, truncation=True)\n",
    "# input_ids = inputs['input_ids']\n",
    "# attention_mask = inputs['attention_mask']\n",
    "# outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "#                              num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "# response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print('prompt: ', prompt_)\n",
    "# print('\\n')\n",
    "# print('bpe encoded response: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46b65d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "\n",
      "\n",
      "char level encoded response:  Please give us creative product ideas related to air. to. like. to. like..........................................................\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using char tokenizer\n",
    "encoding = char_tokenizer.encode(prompt_)\n",
    "input_ids = torch.tensor([encoding.ids], dtype=torch.long).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)  # Dummy attention mask, since char_tokenizer doesn't provide it\n",
    "outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                             num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "response = char_tokenizer.decode(outputs[0].cpu().numpy().tolist())\n",
    "print('prompt: ', prompt_)\n",
    "print('\\n')\n",
    "print('char level encoded response: ', response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1fce81e",
   "metadata": {},
   "source": [
    "Clearly different tokeniser models give different op. clearly meta-lamma-3-8B inbuilt tokeniser works best for given model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cb062",
   "metadata": {},
   "source": [
    "## 4.\tExplain and implement how sentiment analysis can be used to analyze user questions or contexts. [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02586b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\") \n",
    "\n",
    "# A list of example questions is created to demonstrate how sentiment analysis works.\n",
    "user_texts = [\n",
    "    \"I'm really happy with the new product!\",\n",
    "    \"I had a terrible experience with the customer service.\",\n",
    "    \"Can you help me with my issue?\",\n",
    "    \"The latest update is okay, but it could be better.\"\n",
    "]\n",
    "\n",
    "# Each question is analyzed for sentiment, and the results are printed out.\n",
    "for text in user_texts:\n",
    "    sentiment = sentiment_pipeline(text) # It takes a text input, runs it through the sentiment analysis pipeline, and returns the result.\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "861b7ce3",
   "metadata": {},
   "source": [
    "Text: I'm really happy with the new product!\n",
    "Sentiment: [{'label': 'POSITIVE', 'score': 0.999872088432312}]\n",
    "\n",
    "Text: I had a terrible experience with the customer service.\n",
    "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9992349147796631}]\n",
    "\n",
    "Text: Can you help me with my issue?\n",
    "Sentiment: [{'label': 'NEGATIVE', 'score': 0.6718969345092773}]\n",
    "\n",
    "Text: The latest update is okay, but it could be better.\n",
    "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9688122272491455}]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9eff837",
   "metadata": {},
   "source": [
    "The output is a list of sentiments for each user question, indicating whether the sentiment is positive, negative, or neutral, along with a confidence score."
   ]
  },
  {
   "cell_type": "raw",
   "id": "59cd904b",
   "metadata": {},
   "source": [
    "This implementation shows how sentiment analysis can be effectively applied to understand user sentiments in various contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba8c35",
   "metadata": {},
   "source": [
    "## 5.\tImplement fine-tuning for an open-source model to improve the results. [3 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0fa7119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9e7bb3a168444f800e230de97af181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aee2e6b42fe4a96b5b5e10bdbb4e488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1038 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = llm_tokenizer(examples['prompt'], add_special_tokens=True, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    outputs = llm_tokenizer(examples['response'], add_special_tokens=True, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "dataset = load_dataset(\"nvidia/HELPSTEER2\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataset = tokenized_datasets[\"train\"].select(range(20))\n",
    "eval_dataset = tokenized_datasets[\"validation\"].select(range(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed32a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=llm_tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31dfecfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49880c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    use_cpu=True,  # Ensures training is on CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "806c3a72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41fd8bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 13:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.521920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.785708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.745604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "889.6026239395142"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = time.time()\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=llm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "time.time()-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9541f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to sft_finetuned_llm_model.pth\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "model_save_path = \"sft_finetuned_llm_model.pth\"\n",
    "torch.save(llm_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "time.time() - init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f47237",
   "metadata": {},
   "source": [
    "## 6.\tShow 5-10 working examples that show improvements of the accuracy of the fine-tuned model. [3 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original model and finding responses on old and sft trained model on 6 prompts in val df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bdde17c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9016f5cf3e64a27a66d0ab39202dbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m1_tokeniser = AutoTokenizer.from_pretrained(llm_model_name, add_bos_token=False, add_eos_token=True)\n",
    "m1_tokeniser.pad_token = m1_tokeniser.eos_token\n",
    "m1_model = AutoModelForCausalLM.from_pretrained(llm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799db096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0cd84071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "meta lamma tokeniser output:  ['Please', 'Ä give', 'Ä us', 'Ä creative', 'Ä product', 'Ä ideas', 'Ä related', 'Ä to', 'Ä air', 'Ä fresh', 'eners', '.']\n",
      "generated response:  Please give us creative product ideas related to air fresheners.\n"
     ]
    }
   ],
   "source": [
    "def generate_llm_responses(text_):\n",
    "    inputs = llm_tokenizer.encode_plus(text_, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = llm_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                                 num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)\n",
    "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt_ = 'Please give us creative product ideas related to air fresheners.'\n",
    "print('prompt: ', prompt_)\n",
    "print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "975a7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please give us creative product ideas related to air fresheners.\n",
      "meta lamma tokeniser output:  ['Please', 'Ä give', 'Ä us', 'Ä creative', 'Ä product', 'Ä ideas', 'Ä related', 'Ä to', 'Ä air', 'Ä fresh', 'eners', '.']\n",
      "generated response:  Please give us creative product ideas related to air fresheners. We are looking for creative ideas that can be turned into a product.\n",
      "I'm looking for an air freshener with a unique scent. I need it to be something that I can sell to people who have dogs and need to freshen their homes.\n",
      "I am looking for a creative product idea related to air fresheners. I need it to be something that I can sell to people who are looking for a unique way to freshen their homes.\n",
      "I'm looking for a creative product idea related to air fresheners. I need it to be something that I can sell to people who are looking for a unique way to freshen their homes\n"
     ]
    }
   ],
   "source": [
    "def generate_m1_responses(text_):\n",
    "    inputs =m1_tokeniser.encode_plus(text_, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = m1_model.generate(input_ids, attention_mask=attention_mask, max_length=140,\n",
    "                                 num_return_sequences=1, pad_token_id=m1_tokeniser.eos_token_id)\n",
    "    response = m1_tokeniser.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt_ = 'Please give us creative product ideas related to air fresheners.'\n",
    "print('prompt: ', prompt_)\n",
    "print('meta lamma tokeniser output: ', m1_tokeniser.tokenize(prompt_))\n",
    "print('generated response: ', generate_m1_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8306faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36a98eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  what is SoP for studying abroad students\n",
      "\n",
      "generated response:  what is SoP for studying abroad students\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][14]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48a02937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  what is SoP for studying abroad students\n",
      "\n",
      "generated response:  what is SoP for studying abroad students\n",
      "The SoP for studying abroad students is a statement of purpose that helps you to get admission to a university. It is a document that you write to explain why you want to study abroad and what you hope to achieve by doing so. It should be written in a clear and concise manner, and it should be tailored to the specific university you are applying to. The SoP for studying abroad students is an important part of the application process, and it can help you to stand out from other applicants.\n",
      "What is a SoP for studying abroad students?\n",
      "A SoP for studying abroad students is a statement of purpose that helps students to explain their motivation for\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][14]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', m1_tokeniser.tokenize(prompt_))\n",
    "print('generated response: ', generate_m1_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d98c05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81087694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns\n",
      "generated response:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][18]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47d2d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns\n",
      "generated response:  Give me caption ideas that are somber and poetic. The caption is for a sinister looking woman with horns. She is standing in front of a dark, ominous, tree.\n",
      "I think you need to give more context. Is she a demon? A witch? A vampire? A ghost? A zombie? A goth? A succubus? A demoness? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus? A succubus?\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][18]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', m1_tokeniser.tokenize(prompt_))\n",
    "print('generated response: ', generate_m1_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4104784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "992fa8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please provide the equation to calculate the moment of inertia of a trapezium\n",
      "generated response:  Please provide the equation to calculate the moment of inertia of a trapezium\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][24]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc4dc1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Please provide the equation to calculate the moment of inertia of a trapezium\n",
      "generated response:  Please provide the equation to calculate the moment of inertia of a trapezium about the centroidal axis.\n",
      "The moment of inertia of a trapezium about the centroidal axis can be calculated using the formula:\n",
      "I = \\frac{bh^3}{12} + \\frac{dh^3}{12}\n",
      "I = moment of inertia of the trapezium\n",
      "b = base length\n",
      "h = height of the trapezium\n",
      "d = distance from the centroidal axis to the base of the trapezium\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][24]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_m1_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e79bcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd008b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  What are the different components of a business plan\n",
      "generated response:  What are the different components of a business plan\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][60]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "614d0100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  What are the different components of a business plan\n",
      "generated response:  What are the different components of a business plan?\n",
      "What are the different components of a business plan?\n",
      "The key components of a business plan include:\n",
      "What are the 4 elements of a business plan?\n",
      "The four key elements of a business plan are:\n",
      "The Executive Summary. The executive summary is the first section of the business plan.\n",
      "The Company Description.\n",
      "The Market Analysis.\n",
      "The Marketing Plan.\n",
      "The Company Organization.\n",
      "The Service or Product Line.\n",
      "The Marketing Strategy.\n",
      "The Competition Analysis.\n",
      "What is the most important part of a business plan?\n",
      "The executive summary is the most important part of your business plan. It is the first thing that potential investors will see, and it will determine whether they will read\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][60]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_m1_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd5400b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25e80d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Explain Business plan legal use vs operating agreement\n",
      "generated response:  Explain Business plan legal use vs operating agreement\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][110]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_llm_responses(prompt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a681f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  Explain Business plan legal use vs operating agreement\n",
      "generated response:  Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n",
      "Explain Business plan legal use vs operating agreement\n"
     ]
    }
   ],
   "source": [
    "prompt_ = val_df['prompt'][110]\n",
    "print('prompt: ', prompt_)\n",
    "# print('meta lamma tokeniser output: ', llm_tokenizer.tokenize(prompt_))\n",
    "print('generated response: ', generate_m1_responses(prompt_))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "098187f8",
   "metadata": {},
   "source": [
    "Training on 20 datapoints for 3 epoch worsens the performance! train on more/get code reviewed to understand if worsening is expected\n",
    "\n",
    "Check if finding ROUGE here is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45db978",
   "metadata": {},
   "source": [
    "## 7.\tApply and fine-tune the generator model using Parameter-Efficient Fine-Tuning (PEFT). [3 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283681a",
   "metadata": {},
   "source": [
    "## 8.\tImplement a function to evaluate the model's performance using metrics such as accuracy and F1 score. [1 Mark]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3a17e",
   "metadata": {},
   "source": [
    "## 9.\tAnalyze the results to discuss the impact of PEFT on model performance and efficiency. [1 Mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5cb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
